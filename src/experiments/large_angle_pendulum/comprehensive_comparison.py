#!/usr/bin/env python3
"""
å¤§è§’åº¦å•æ‘†æ–¹æ³•ç»¼åˆå¯¹æ¯” / Comprehensive Large Angle Pendulum Method Comparison
=============================================================================

å¯¹æ¯”å››ç§æ–¹æ³•åœ¨å¤§è§’åº¦å•æ‘†ç³»ç»Ÿä¸Šçš„æ€§èƒ½ï¼š
1. MMSB-VI (Multi-Marginal SchrÃ¶dinger Bridge Variational Inference)
2. EKF (Extended Kalman Filter)  
3. UKF (Unscented Kalman Filter)
4. SVI (Stochastic Variational Inference)

ä½¿ç”¨æ¦‚ç‡å¯†åº¦è´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼š
- Negative Log-Likelihood (NLL)
- 95% Credible Coverage
- Bimodality Detection

Compare four methods on large angle pendulum system:
1. MMSB-VI 
2. EKF
3. UKF
4. SVI

Using probability density quality metrics:
- Negative Log-Likelihood (NLL)
- 95% Credible Coverage  
- Bimodality Detection
"""

import jax
import jax.numpy as jnp
import numpy as np
import time
import sys
import pathlib
from typing import Dict, List, Tuple, Any
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
root_dir = pathlib.Path(__file__).resolve().parents[3]
if str(root_dir) not in sys.path:
    sys.path.append(str(root_dir))

jax.config.update('jax_enable_x64', True)

# å¯¼å…¥æˆ‘ä»¬çš„ç»„ä»¶
from data_generator import (
    LargeAnglePendulumGenerator, PendulumParams, ObservationConfig
)
from pendulum_mmsb_solver import (
    PendulumMMSBSolver, PendulumMMSBConfig
)
from evaluation_metrics import (
    DensityQualityMetrics, DensityEstimate
)
from src.baselines import (
    PendulumEKFSmoother, PendulumUKFSmoother, PendulumSVISmoother
)


class ComprehensiveComparison:
    """
    å¤§è§’åº¦å•æ‘†æ–¹æ³•ç»¼åˆå¯¹æ¯”å™¨ / Comprehensive pendulum method comparator
    """
    
    def __init__(self, 
                 scenario_name: str = "inverted_equilibrium",
                 random_seed: int = 42):
        """
        åˆå§‹åŒ–å¯¹æ¯”å™¨ / Initialize comparator
        
        Args:
            scenario_name: æµ‹è¯•åœºæ™¯åç§° / test scenario name
            random_seed: éšæœºç§å­ / random seed
        """
        self.scenario_name = scenario_name
        self.seed = random_seed
        self.key = jax.random.PRNGKey(random_seed)
        
        # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨ / Create data generator
        self.pendulum_params = PendulumParams(
            g=9.81,
            L=1.0, 
            gamma=0.2,
            sigma=0.3
        )
        
        # åˆ›å»ºè§‚æµ‹æ—¶åˆ» / Create observation times
        obs_times = jnp.arange(0.0, 3.0, 0.1)  # æ¯0.1sè§‚æµ‹ä¸€æ¬¡ï¼Œæ€»å…±3s
        
        self.obs_config = ObservationConfig(
            obs_times=obs_times,
            obs_noise_std=0.1,
            sparse_strategy="skip_unstable"
        )
        
        self.data_generator = LargeAnglePendulumGenerator(
            params=self.pendulum_params,
            dt=0.05,
            total_time=3.0
        )
        
        # åˆ›å»ºæ±‚è§£å™¨ / Create solvers
        self._setup_solvers()
        
        # åˆ›å»ºè¯„ä¼°å™¨ / Create evaluator
        self.evaluator = DensityQualityMetrics()
        
        print(f"ğŸ¯ å¤§è§’åº¦å•æ‘†ç»¼åˆå¯¹æ¯”åˆå§‹åŒ–å®Œæˆ")
        print(f"   åœºæ™¯: {scenario_name}")
        print(f"   éšæœºç§å­: {random_seed}")
        print(f"   ç‰©ç†å‚æ•°: g={self.pendulum_params.g}, L={self.pendulum_params.L}")
        print(f"   å™ªå£°å‚æ•°: Î³={self.pendulum_params.gamma}, Ïƒ={self.pendulum_params.sigma}")
    
    def _setup_solvers(self):
        """è®¾ç½®æ‰€æœ‰æ±‚è§£å™¨ / Setup all solvers"""
        
        # MMSB-VIé…ç½® / MMSB-VI configuration
        self.mmsb_config = PendulumMMSBConfig(
            theta_grid_points=64,
            omega_grid_points=32,
            ipfp_max_iterations=300,
            ipfp_tolerance=1e-6
        )
        self.mmsb_solver = PendulumMMSBSolver(self.mmsb_config)
        
        # EKFé…ç½® / EKF configuration
        self.ekf_smoother = PendulumEKFSmoother(
            dt=self.data_generator.dt,
            g=self.pendulum_params.g,
            L=self.pendulum_params.L,
            gamma=self.pendulum_params.gamma,
            sigma=self.pendulum_params.sigma,
            obs_noise_std=self.obs_config.obs_noise_std
        )
        
        # UKFé…ç½® / UKF configuration
        self.ukf_smoother = PendulumUKFSmoother(
            dt=self.data_generator.dt,
            g=self.pendulum_params.g,
            L=self.pendulum_params.L,
            gamma=self.pendulum_params.gamma,
            sigma=self.pendulum_params.sigma,
            obs_noise_std=self.obs_config.obs_noise_std,
            alpha=1.0,
            beta=2.0,
            kappa=1.0
        )
        
        # SVIé…ç½® / SVI configuration
        self.svi_smoother = PendulumSVISmoother(
            dt=self.data_generator.dt,
            g=self.pendulum_params.g,
            L=self.pendulum_params.L,
            gamma=self.pendulum_params.gamma,
            sigma=self.pendulum_params.sigma,
            obs_noise_std=self.obs_config.obs_noise_std,
            learning_rate=0.01,
            n_samples=20,
            max_iterations=1000,
            convergence_tol=1e-6
        )
    
    def generate_test_scenario(self) -> Any:
        """
        ç”Ÿæˆæµ‹è¯•åœºæ™¯æ•°æ® / Generate test scenario data
        
        Returns:
            trajectory: å•æ‘†è½¨è¿¹æ•°æ® / pendulum trajectory data
        """
        if self.scenario_name == "inverted_equilibrium":
            # å€’ç«‹ç‚¹é™„è¿‘çš„å…³é”®åœºæ™¯ / Critical scenario near inverted point
            initial_theta = jnp.pi - 0.1 + 0.05 * jax.random.normal(self.key)
            initial_omega = 0.0 + 0.1 * jax.random.normal(self.key)
            initial_state = jnp.array([initial_theta, initial_omega])
            
        elif self.scenario_name == "large_swing":
            # å¤§å¹…æ‘†åŠ¨åœºæ™¯ / Large swing scenario
            initial_theta = jnp.pi/3 + 0.1 * jax.random.normal(self.key)
            initial_omega = 2.0 + 0.2 * jax.random.normal(self.key)
            initial_state = jnp.array([initial_theta, initial_omega])
            
        else:
            # é»˜è®¤å°è§’åº¦åœºæ™¯ / Default small angle scenario
            initial_theta = 0.2 + 0.05 * jax.random.normal(self.key)
            initial_omega = 0.0 + 0.1 * jax.random.normal(self.key)
            initial_state = jnp.array([initial_theta, initial_omega])
        
        print(f"\nğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®...")
        print(f"   åˆå§‹çŠ¶æ€: Î¸â‚€={initial_state[0]:.3f}, Ï‰â‚€={initial_state[1]:.3f}")
        
        # ç”Ÿæˆè½¨è¿¹ / Generate trajectory
        if self.scenario_name == "inverted_equilibrium":
            # ä½¿ç”¨å†…ç½®çš„å€’ç«‹ç‚¹åœºæ™¯ç”Ÿæˆå™¨
            trajectory = self.data_generator.generate_unstable_scenario(
                key=self.key,
                theta_perturbation=0.05,
                omega_perturbation=0.02
            )
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰åˆå§‹çŠ¶æ€ç”Ÿæˆè½¨è¿¹
            trajectory = self.data_generator._generate_trajectory_impl(
                initial_state, self.key, self.obs_config
            )
        
        print(f"   è§‚æµ‹æ•°é‡: {len(trajectory.observations)}")
        print(f"   æ—¶é—´èŒƒå›´: {trajectory.obs_times[0]:.2f} - {trajectory.obs_times[-1]:.2f}s")
        
        return trajectory
    
    def run_mmsb_vi(self, trajectory) -> Tuple[Any, Dict[str, float]]:
        """è¿è¡ŒMMSB-VIæ–¹æ³• / Run MMSB-VI method"""
        print(f"\nğŸ”¬ è¿è¡ŒMMSB-VI...")
        
        start_time = time.time()
        try:
            result = self.mmsb_solver.solve(trajectory, verbose=False)
            runtime = time.time() - start_time
            
            # è¯„ä¼°æ€§èƒ½ / Evaluate performance
            # åˆ›å»ºæ—¶åˆ»ç´¢å¼• / Create time indices
            time_indices = jnp.arange(len(result.density_estimates))
            
            quality_metrics = self.evaluator.evaluate_density_sequence(
                result.density_estimates, 
                trajectory.states,
                time_indices
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ / Convert to dictionary format
            metrics = {
                'nll_mean': float(jnp.mean(quality_metrics.nll_per_time)),
                'nll_std': float(jnp.std(quality_metrics.nll_per_time)),
                'coverage_95': float(quality_metrics.coverage_95),
                'bimodality_score': float(quality_metrics.theta_ks_statistic),
                'runtime': runtime,
                'converged': result.convergence_info['converged'],
                'n_iterations': result.convergence_info['n_iterations']
            }
            
            print(f"âœ… MMSB-VIå®Œæˆ: {runtime:.2f}s, æ”¶æ•›: {result.convergence_info['converged']}")
            
            return result, metrics
            
        except Exception as e:
            print(f"âŒ MMSB-VIå¤±è´¥: {e}")
            return None, {'runtime': time.time() - start_time, 'error': str(e)}
    
    def run_ekf(self, trajectory) -> Tuple[Any, Dict[str, float]]:
        """è¿è¡ŒEKFæ–¹æ³• / Run EKF method"""
        print(f"\nğŸ”¬ è¿è¡ŒEKF...")
        
        start_time = time.time()
        try:
            # è®¾ç½®åˆå§‹æ¡ä»¶ / Set initial conditions
            initial_mean = jnp.array([trajectory.observations[0], 0.0])
            initial_cov = jnp.eye(2) * 0.5
            
            result = self.ekf_smoother.smooth(
                trajectory.observations,
                initial_mean=initial_mean,
                initial_cov=initial_cov
            )
            runtime = time.time() - start_time
            
            # è½¬æ¢ä¸ºå¯†åº¦ä¼°è®¡æ ¼å¼ / Convert to density estimate format
            density_estimates = self._convert_gaussian_to_densities(
                result.smoothed_states,
                trajectory.obs_times,
                "EKF"
            )
            
            # è¯„ä¼°æ€§èƒ½ / Evaluate performance
            # åˆ›å»ºæ—¶åˆ»ç´¢å¼• / Create time indices
            time_indices = jnp.arange(len(density_estimates))
            
            quality_metrics = self.evaluator.evaluate_density_sequence(
                density_estimates,
                trajectory.states,
                time_indices
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ / Convert to dictionary format
            metrics = {
                'nll_mean': float(jnp.mean(quality_metrics.nll_per_time)),
                'nll_std': float(jnp.std(quality_metrics.nll_per_time)),
                'coverage_95': float(quality_metrics.coverage_95),
                'bimodality_score': float(quality_metrics.theta_ks_statistic),
                'runtime': runtime,
                'log_likelihood': result.total_log_likelihood
            }
            
            print(f"âœ… EKFå®Œæˆ: {runtime:.2f}s")
            
            return result, metrics
            
        except Exception as e:
            print(f"âŒ EKFå¤±è´¥: {e}")
            return None, {'runtime': time.time() - start_time, 'error': str(e)}
    
    def run_ukf(self, trajectory) -> Tuple[Any, Dict[str, float]]:
        """è¿è¡ŒUKFæ–¹æ³• / Run UKF method"""
        print(f"\nğŸ”¬ è¿è¡ŒUKF...")
        
        start_time = time.time()
        try:
            # è®¾ç½®åˆå§‹æ¡ä»¶ / Set initial conditions
            initial_mean = jnp.array([trajectory.observations[0], 0.0])
            initial_cov = jnp.eye(2) * 0.5
            
            result = self.ukf_smoother.smooth(
                trajectory.observations,
                initial_mean=initial_mean,
                initial_cov=initial_cov
            )
            runtime = time.time() - start_time
            
            # è½¬æ¢ä¸ºå¯†åº¦ä¼°è®¡æ ¼å¼ / Convert to density estimate format
            density_estimates = self._convert_gaussian_to_densities(
                result.smoothed_states,
                trajectory.obs_times,
                "UKF"
            )
            
            # è¯„ä¼°æ€§èƒ½ / Evaluate performance
            # åˆ›å»ºæ—¶åˆ»ç´¢å¼• / Create time indices
            time_indices = jnp.arange(len(density_estimates))
            
            quality_metrics = self.evaluator.evaluate_density_sequence(
                density_estimates,
                trajectory.states,
                time_indices
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ / Convert to dictionary format
            metrics = {
                'nll_mean': float(jnp.mean(quality_metrics.nll_per_time)),
                'nll_std': float(jnp.std(quality_metrics.nll_per_time)),
                'coverage_95': float(quality_metrics.coverage_95),
                'bimodality_score': float(quality_metrics.theta_ks_statistic),
                'runtime': runtime,
                'log_likelihood': result.total_log_likelihood
            }
            
            print(f"âœ… UKFå®Œæˆ: {runtime:.2f}s")
            
            return result, metrics
            
        except Exception as e:
            print(f"âŒ UKFå¤±è´¥: {e}")
            return None, {'runtime': time.time() - start_time, 'error': str(e)}
    
    def run_svi(self, trajectory) -> Tuple[Any, Dict[str, float]]:
        """è¿è¡ŒSVIæ–¹æ³• / Run SVI method"""
        print(f"\nğŸ”¬ è¿è¡ŒSVI...")
        
        start_time = time.time()
        try:
            # è®¾ç½®åˆå§‹æ¡ä»¶ / Set initial conditions
            initial_mean = jnp.array([trajectory.observations[0], 0.0])
            initial_cov = jnp.eye(2) * 0.5
            
            result = self.svi_smoother.smooth(
                trajectory.observations,
                initial_mean=initial_mean,
                initial_cov=initial_cov
            )
            runtime = time.time() - start_time
            
            # è½¬æ¢ä¸ºå¯†åº¦ä¼°è®¡æ ¼å¼ / Convert to density estimate format
            density_estimates = self._convert_svi_to_densities(
                result,
                trajectory.obs_times,
                "SVI"
            )
            
            # è¯„ä¼°æ€§èƒ½ / Evaluate performance
            # åˆ›å»ºæ—¶åˆ»ç´¢å¼• / Create time indices
            time_indices = jnp.arange(len(density_estimates))
            
            quality_metrics = self.evaluator.evaluate_density_sequence(
                density_estimates,
                trajectory.states,
                time_indices
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ / Convert to dictionary format
            metrics = {
                'nll_mean': float(jnp.mean(quality_metrics.nll_per_time)),
                'nll_std': float(jnp.std(quality_metrics.nll_per_time)),
                'coverage_95': float(quality_metrics.coverage_95),
                'bimodality_score': float(quality_metrics.theta_ks_statistic),
                'runtime': runtime,
                'elbo': float(result.elbo),
                'log_likelihood': float(result.total_log_likelihood)
            }
            
            print(f"âœ… SVIå®Œæˆ: {runtime:.2f}s, ELBO: {result.elbo:.3f}")
            
            return result, metrics
            
        except Exception as e:
            print(f"âŒ SVIå¤±è´¥: {e}")
            return None, {'runtime': time.time() - start_time, 'error': str(e)}
    
    def _convert_svi_to_densities(self, svi_result, obs_times, method_name):
        """å°†SVIç»“æœè½¬æ¢ä¸ºå¯†åº¦ä¼°è®¡æ ¼å¼ / Convert SVI result to density estimates"""
        
        # ä½¿ç”¨ä¸MMSB-VIç›¸åŒçš„ç½‘æ ¼ / Use same grid as MMSB-VI
        theta_grid = self.mmsb_solver.theta_grid
        omega_grid = self.mmsb_solver.omega_grid
        
        density_estimates = []
        
        for t in range(len(svi_result.means)):
            # è·å–å˜åˆ†å‚æ•° / Get variational parameters
            mean = svi_result.means[t]
            std = jnp.exp(svi_result.log_stds[t])
            cov = jnp.diag(std**2)
            
            # åˆ›å»º2Dç½‘æ ¼ / Create 2D grid
            theta_2d, omega_2d = jnp.meshgrid(theta_grid, omega_grid, indexing='ij')
            
            # è®¡ç®—2Dé«˜æ–¯å¯†åº¦ï¼ˆè€ƒè™‘å‘¨æœŸæ€§ï¼‰/ Compute 2D Gaussian density (considering periodicity)
            density_2d = self._compute_periodic_gaussian_density(
                theta_2d, omega_2d, mean, cov
            )
            
            # è®¡ç®—è¾¹é™…åˆ†å¸ƒ / Compute marginal distributions
            dtheta = theta_grid[1] - theta_grid[0]
            domega = omega_grid[1] - omega_grid[0]
            
            # ä½¿ç”¨è‡ªå®šä¹‰æ¢¯å½¢ç§¯åˆ†æ›¿ä»£jnp.trapz
            domega = omega_grid[1] - omega_grid[0]
            marginal_theta = domega * (jnp.sum(density_2d, axis=1) - 0.5 * (density_2d[:, 0] + density_2d[:, -1]))
            
            dtheta = theta_grid[1] - theta_grid[0]
            marginal_omega = dtheta * (jnp.sum(density_2d, axis=0) - 0.5 * (density_2d[0, :] + density_2d[-1, :]))
            
            # åˆ›å»ºå¯†åº¦ä¼°è®¡ / Create density estimate
            density_estimate = DensityEstimate(
                theta_grid=theta_grid,
                omega_grid=omega_grid,
                density_2d=density_2d,
                marginal_theta=marginal_theta,
                marginal_omega=marginal_omega,
                time_index=t,
                log_likelihood=float(svi_result.total_log_likelihood / len(svi_result.means))  # å¹³å‡æ¯æ—¶åˆ»çš„å¯¹æ•°ä¼¼ç„¶
            )
            
            density_estimates.append(density_estimate)
        
        return density_estimates
    
    def _convert_gaussian_to_densities(self, gaussian_states, obs_times, method_name):
        """å°†é«˜æ–¯çŠ¶æ€è½¬æ¢ä¸ºå¯†åº¦ä¼°è®¡æ ¼å¼ / Convert Gaussian states to density estimates"""
        
        # ä½¿ç”¨ä¸MMSB-VIç›¸åŒçš„ç½‘æ ¼ / Use same grid as MMSB-VI
        theta_grid = self.mmsb_solver.theta_grid
        omega_grid = self.mmsb_solver.omega_grid
        
        density_estimates = []
        
        for t, state in enumerate(gaussian_states):
            mean = state.mean
            cov = state.covariance
            
            # åˆ›å»º2Dç½‘æ ¼ / Create 2D grid
            theta_2d, omega_2d = jnp.meshgrid(theta_grid, omega_grid, indexing='ij')
            
            # è®¡ç®—2Dé«˜æ–¯å¯†åº¦ï¼ˆè€ƒè™‘å‘¨æœŸæ€§ï¼‰/ Compute 2D Gaussian density (considering periodicity)
            density_2d = self._compute_periodic_gaussian_density(
                theta_2d, omega_2d, mean, cov
            )
            
            # è®¡ç®—è¾¹é™…åˆ†å¸ƒ / Compute marginal distributions
            dtheta = theta_grid[1] - theta_grid[0]
            domega = omega_grid[1] - omega_grid[0]
            
            # ä½¿ç”¨è‡ªå®šä¹‰æ¢¯å½¢ç§¯åˆ†æ›¿ä»£jnp.trapz
            domega = omega_grid[1] - omega_grid[0]
            marginal_theta = domega * (jnp.sum(density_2d, axis=1) - 0.5 * (density_2d[:, 0] + density_2d[:, -1]))
            
            dtheta = theta_grid[1] - theta_grid[0]
            marginal_omega = dtheta * (jnp.sum(density_2d, axis=0) - 0.5 * (density_2d[0, :] + density_2d[-1, :]))
            
            # åˆ›å»ºå¯†åº¦ä¼°è®¡ / Create density estimate
            density_estimate = DensityEstimate(
                theta_grid=theta_grid,
                omega_grid=omega_grid,
                density_2d=density_2d,
                marginal_theta=marginal_theta,
                marginal_omega=marginal_omega,
                time_index=t,
                log_likelihood=float(state.log_likelihood)
            )
            
            density_estimates.append(density_estimate)
        
        return density_estimates
    
    def _compute_periodic_gaussian_density(self, theta_2d, omega_2d, mean, cov):
        """è®¡ç®—è€ƒè™‘å‘¨æœŸæ€§çš„2Dé«˜æ–¯å¯†åº¦ / Compute 2D Gaussian density considering periodicity"""
        # è®¡ç®—åˆ°å‡å€¼çš„è·ç¦»ï¼ˆè€ƒè™‘Î¸çš„å‘¨æœŸæ€§ï¼‰/ Compute distance to mean (considering Î¸ periodicity)
        theta_diff = theta_2d - mean[0]
        # å¤„ç†è§’åº¦å·®çš„å‘¨æœŸæ€§ / Handle periodicity of angle difference
        theta_diff = jnp.mod(theta_diff + jnp.pi, 2 * jnp.pi) - jnp.pi
        
        omega_diff = omega_2d - mean[1]
        
        # æ„é€ å·®å€¼å‘é‡ / Construct difference vector
        diff = jnp.stack([theta_diff, omega_diff], axis=-1)
        
        # ç¡®ä¿åæ–¹å·®çŸ©é˜µæ•°å€¼ç¨³å®š / Ensure numerical stability of covariance matrix
        cov_stable = cov + jnp.eye(2) * 1e-6
        cov_inv = jnp.linalg.inv(cov_stable)
        cov_det = jnp.linalg.det(cov_stable)
        
        # è®¡ç®—é©¬æ°è·ç¦» / Compute Mahalanobis distance
        mahalanobis_sq = jnp.sum(diff @ cov_inv * diff, axis=-1)
        
        # 2Dé«˜æ–¯å¯†åº¦ / 2D Gaussian density
        density_2d = jnp.exp(-0.5 * mahalanobis_sq) / (2 * jnp.pi * jnp.sqrt(jnp.maximum(cov_det, 1e-12)))
        
        return density_2d
    
    def run_comprehensive_comparison(self) -> pd.DataFrame:
        """
        è¿è¡Œç»¼åˆå¯¹æ¯” / Run comprehensive comparison
        
        Returns:
            results_df: ç»“æœæ•°æ®æ¡† / results dataframe
        """
        print("="*60)
        print("ğŸ¯ å¼€å§‹å¤§è§’åº¦å•æ‘†æ–¹æ³•ç»¼åˆå¯¹æ¯”")
        print("ğŸ¯ Starting Comprehensive Large Angle Pendulum Comparison")
        print("="*60)
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ® / Generate test data
        trajectory = self.generate_test_scenario()
        
        # è¿è¡Œæ‰€æœ‰æ–¹æ³• / Run all methods
        results = {}
        
        # MMSB-VI
        mmsb_result, mmsb_metrics = self.run_mmsb_vi(trajectory)
        results['MMSB-VI'] = mmsb_metrics
        
        # EKF
        ekf_result, ekf_metrics = self.run_ekf(trajectory)
        results['EKF'] = ekf_metrics
        
        # UKF
        ukf_result, ukf_metrics = self.run_ukf(trajectory)
        results['UKF'] = ukf_metrics
        
        # SVI
        svi_result, svi_metrics = self.run_svi(trajectory)
        results['SVI'] = svi_metrics
        
        # æ•´ç†ç»“æœ / Organize results
        results_df = self._create_results_dataframe(results)
        
        # æ˜¾ç¤ºç»“æœ / Display results
        self._display_results(results_df)
        
        return results_df
    
    def _create_results_dataframe(self, results: Dict) -> pd.DataFrame:
        """åˆ›å»ºç»“æœæ•°æ®æ¡† / Create results dataframe"""
        
        # å®šä¹‰å…³é”®æŒ‡æ ‡ / Define key metrics
        key_metrics = [
            'nll_mean', 'nll_std',
            'coverage_95', 
            'bimodality_score',
            'runtime',
        ]
        
        # åˆ›å»ºæ•°æ®æ¡† / Create dataframe
        df_data = []
        for method, metrics in results.items():
            row = {'Method': method}
            for metric in key_metrics:
                if metric in metrics:
                    row[metric] = metrics[metric]
                else:
                    row[metric] = np.nan
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯ / Add extra info
            if 'error' in metrics:
                row['Status'] = f"Error: {metrics['error']}"
            else:
                row['Status'] = "Success"
                
            df_data.append(row)
        
        return pd.DataFrame(df_data)
    
    def _display_results(self, results_df: pd.DataFrame):
        """æ˜¾ç¤ºå¯¹æ¯”ç»“æœ / Display comparison results"""
        
        print("\n" + "="*80)
        print("ğŸ“Š ç»¼åˆå¯¹æ¯”ç»“æœ / Comprehensive Comparison Results")
        print("="*80)
        
        # æ˜¾ç¤ºåŸºæœ¬æ€§èƒ½è¡¨ / Display basic performance table
        print("\nğŸ† æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” / Performance Metrics Comparison:")
        print("-" * 80)
        
        # æ ¼å¼åŒ–æ˜¾ç¤º / Formatted display
        for _, row in results_df.iterrows():
            method = row['Method']
            status = row['Status']
            
            if status == "Success":
                nll_mean = row['nll_mean']
                coverage = row['coverage_95']
                bimodality = row['bimodality_score']
                runtime = row['runtime']
                
                print(f"{method:>10} | NLL: {nll_mean:6.3f} | Coverage: {coverage:5.1%} | Bimodality: {bimodality:5.3f} | Time: {runtime:6.2f}s")
            else:
                print(f"{method:>10} | {status}")
        
        print("-" * 80)
        
        # åˆ†ææœ€ä½³æ–¹æ³• / Analyze best methods
        success_df = results_df[results_df['Status'] == 'Success']
        
        if len(success_df) > 0:
            print("\nğŸ–ï¸  æœ€ä½³æ€§èƒ½åˆ†æ / Best Performance Analysis:")
            
            # NLLæœ€ä½ï¼ˆå¯†åº¦è´¨é‡æœ€å¥½ï¼‰/ Lowest NLL (best density quality)
            best_nll_idx = success_df['nll_mean'].idxmin()
            best_nll_method = success_df.loc[best_nll_idx, 'Method']
            print(f"   æœ€ä½³å¯†åº¦è´¨é‡ (æœ€ä½NLL): {best_nll_method} ({success_df.loc[best_nll_idx, 'nll_mean']:.3f})")
            
            # Coverageæœ€é«˜ï¼ˆæ ¡å‡†æœ€å¥½ï¼‰/ Highest coverage (best calibration)
            best_coverage_idx = success_df['coverage_95'].idxmax()
            best_coverage_method = success_df.loc[best_coverage_idx, 'Method']
            print(f"   æœ€ä½³æ ¡å‡†è´¨é‡ (æœ€é«˜Coverage): {best_coverage_method} ({success_df.loc[best_coverage_idx, 'coverage_95']:.1%})")
            
            # Bimodalityæœ€é«˜ï¼ˆå¤šæ¨¡æ€æ£€æµ‹æœ€å¥½ï¼‰/ Highest bimodality (best multimodal detection)
            best_bimodal_idx = success_df['bimodality_score'].idxmax()
            best_bimodal_method = success_df.loc[best_bimodal_idx, 'Method']
            print(f"   æœ€ä½³å¤šæ¨¡æ€æ£€æµ‹: {best_bimodal_method} ({success_df.loc[best_bimodal_idx, 'bimodality_score']:.3f})")
            
            # æœ€å¿«é€Ÿåº¦ / Fastest runtime
            fastest_idx = success_df['runtime'].idxmin()
            fastest_method = success_df.loc[fastest_idx, 'Method']
            print(f"   æœ€å¿«è¿è¡Œé€Ÿåº¦: {fastest_method} ({success_df.loc[fastest_idx, 'runtime']:.2f}s)")
        
        print("\n" + "="*80)
        print("ğŸ’¡ åˆ†æè¦ç‚¹ / Key Analysis Points:")
        print("   - NLLè¶Šä½è¡¨ç¤ºå¯†åº¦ä¼°è®¡è´¨é‡è¶Šå¥½")
        print("   - Coverageè¶Šæ¥è¿‘95%è¡¨ç¤ºä¸ç¡®å®šæ€§æ ¡å‡†è¶Šå‡†ç¡®")
        print("   - Bimodality Scoreè¶Šé«˜è¡¨ç¤ºå¤šæ¨¡æ€æ£€æµ‹èƒ½åŠ›è¶Šå¼º")
        print("   - è¿™äº›æŒ‡æ ‡æ›´å…¬å¹³åœ°è¯„ä¼°æ¦‚ç‡å»ºæ¨¡è´¨é‡ï¼Œè€Œéä»…ç‚¹ä¼°è®¡ç²¾åº¦")
        print("="*80)


def main():
    """ä¸»å‡½æ•° / Main function"""
    
    # åˆ›å»ºå¯¹æ¯”å™¨ / Create comparator
    comparator = ComprehensiveComparison(
        scenario_name="inverted_equilibrium",  # å…³é”®çš„å€’ç«‹ç‚¹åœºæ™¯
        random_seed=42
    )
    
    # è¿è¡Œå¯¹æ¯” / Run comparison
    results_df = comparator.run_comprehensive_comparison()
    
    # ä¿å­˜ç»“æœ / Save results
    output_file = root_dir / "results" / "pendulum_comparison_results.csv"
    output_file.parent.mkdir(exist_ok=True)
    results_df.to_csv(output_file, index=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return results_df


if __name__ == "__main__":
    results = main()