"""
æ¦‚ç‡å¯†åº¦è´¨é‡è¯„ä¼°æŒ‡æ ‡ / Probability Density Quality Assessment Metrics
================================================================

å®ç°å…¬æ­£çš„æ¦‚ç‡å»ºæ¨¡è´¨é‡è¯„ä¼°ï¼Œä¸“æ³¨äºå¯†åº¦ä¼°è®¡è€Œéç‚¹ä¼°è®¡ï¼š
Implement fair probabilistic modeling quality assessment focused on density estimation rather than point estimation:

1. Negative Log-Likelihood (NLL) - çœŸå®è½¨è¿¹åœ¨ä¼°è®¡å¯†åº¦ä¸‹çš„å¯¹æ•°ä¼¼ç„¶
2. 95% Credible Coverage - éå‚æ•°å¯†åº¦ç§¯åˆ†çš„ç½®ä¿¡åŒºé—´è¦†ç›–
3. Bimodality Significance - å¤šæ¨¡æ€æ˜¾è‘—æ€§ç»Ÿè®¡æ£€éªŒ

è¿™äº›æŒ‡æ ‡èƒ½å…¬æ­£è¯„ä¼°MMSB-VIçš„å¤šæ¨¡æ€å»ºæ¨¡ä¼˜åŠ¿ã€‚
These metrics fairly assess MMSB-VI's multi-modal modeling advantages.
"""

import jax
import jax.numpy as jnp
import numpy as np
from jax import random
from typing import Dict, List, Tuple, Optional, NamedTuple
import chex
from functools import partial
from scipy import stats
from scipy.interpolate import RegularGridInterpolator

jax.config.update('jax_enable_x64', True)


class DensityEstimate(NamedTuple):
    """å¯†åº¦ä¼°è®¡ç»“æœ / Density estimation result"""
    theta_grid: chex.Array      # è§’åº¦ç½‘æ ¼ / angle grid
    omega_grid: chex.Array      # è§’é€Ÿåº¦ç½‘æ ¼ / angular velocity grid  
    density_2d: chex.Array      # 2Då¯†åº¦ (n_theta, n_omega) / 2D density
    marginal_theta: chex.Array  # Î¸è¾¹é™…å¯†åº¦ / Î¸ marginal density
    marginal_omega: chex.Array  # Ï‰è¾¹é™…å¯†åº¦ / Ï‰ marginal density
    time_index: int             # æ—¶åˆ»ç´¢å¼• / time index
    log_likelihood: float       # å¯¹æ•°ä¼¼ç„¶ / log likelihood


class QualityMetrics(NamedTuple):
    """è´¨é‡è¯„ä¼°æŒ‡æ ‡ / Quality assessment metrics"""
    nll_total: float           # æ€»è´Ÿå¯¹æ•°ä¼¼ç„¶ / total negative log-likelihood
    nll_per_time: chex.Array   # æ¯æ—¶åˆ»NLL / per-time NLL
    coverage_95: float         # 95%è¦†ç›–ç‡ / 95% coverage rate
    coverage_per_time: chex.Array  # æ¯æ—¶åˆ»è¦†ç›–ç‡ / per-time coverage
    bimodality_p_value: float  # åŒæ¨¡æ€på€¼ / bimodality p-value
    bimodality_detected: bool  # æ˜¯å¦æ£€æµ‹åˆ°åŒæ¨¡æ€ / bimodality detected
    theta_ks_statistic: float  # Î¸è¾¹é™…KSç»Ÿè®¡é‡ / Î¸ marginal KS statistic
    effective_sample_size: float  # æœ‰æ•ˆæ ·æœ¬æ•° / effective sample size


class DensityQualityMetrics:
    """
    æ¦‚ç‡å¯†åº¦è´¨é‡è¯„ä¼°å™¨ / Probability density quality assessor
    
    ä¸“ä¸ºMMSB-VI vs ç»å…¸æ–¹æ³•çš„å…¬æ­£æ¯”è¾ƒè®¾è®¡ã€‚
    Designed for fair comparison between MMSB-VI and classical methods.
    
    æ ¸å¿ƒç†å¿µï¼šè¯„ä¼°æ¦‚ç‡å¯†åº¦å»ºæ¨¡è´¨é‡ï¼Œè€Œéç®€å•çš„ç‚¹ä¼°è®¡ç²¾åº¦ã€‚
    Core philosophy: assess probabilistic modeling quality, not just point estimation accuracy.
    """
    
    def __init__(
        self,
        theta_bounds: Tuple[float, float] = (-jnp.pi, jnp.pi),
        omega_bounds: Tuple[float, float] = (-6.0, 6.0),
        grid_resolution: Tuple[int, int] = (64, 32),
        confidence_level: float = 0.95
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ / Initialize assessor
        
        Args:
            theta_bounds: è§’åº¦èŒƒå›´ / angle bounds
            omega_bounds: è§’é€Ÿåº¦èŒƒå›´ / angular velocity bounds
            grid_resolution: ç½‘æ ¼åˆ†è¾¨ç‡ (n_theta, n_omega) / grid resolution
            confidence_level: ç½®ä¿¡æ°´å¹³ / confidence level
        """
        self.theta_bounds = theta_bounds
        self.omega_bounds = omega_bounds
        self.grid_resolution = grid_resolution
        self.confidence_level = confidence_level
        
        # åˆ›å»ºè¯„ä¼°ç½‘æ ¼ / Create evaluation grids
        self.theta_grid = jnp.linspace(theta_bounds[0], theta_bounds[1], grid_resolution[0])
        self.omega_grid = jnp.linspace(omega_bounds[0], omega_bounds[1], grid_resolution[1])
        self.dtheta = self.theta_grid[1] - self.theta_grid[0]
        self.domega = self.omega_grid[1] - self.omega_grid[0]
        
        # ç¼–è¯‘æ ¸å¿ƒå‡½æ•° / Compile core functions
        self._compute_nll_at_time = jax.jit(self._compute_nll_at_time_impl)
        self._compute_coverage_at_time = jax.jit(self._compute_coverage_at_time_impl)
    
    def evaluate_density_sequence(
        self,
        density_estimates: List[DensityEstimate],
        true_trajectory: chex.Array,  # (T, 2) [Î¸, Ï‰]
        time_indices: chex.Array      # å¯¹åº”çš„æ—¶åˆ»ç´¢å¼• / corresponding time indices
    ) -> QualityMetrics:
        """
        è¯„ä¼°å¯†åº¦åºåˆ—çš„è´¨é‡ / Evaluate quality of density sequence
        
        Args:
            density_estimates: å¯†åº¦ä¼°è®¡åˆ—è¡¨ / list of density estimates
            true_trajectory: çœŸå®è½¨è¿¹ / true trajectory  
            time_indices: æ—¶åˆ»ç´¢å¼• / time indices
            
        Returns:
            metrics: è´¨é‡è¯„ä¼°æŒ‡æ ‡ / quality assessment metrics
        """
        T = len(density_estimates)
        
        # 1. è®¡ç®—æ¯æ—¶åˆ»çš„NLL / Compute per-time NLL
        nll_per_time = []
        for i, density_est in enumerate(density_estimates):
            true_state = true_trajectory[time_indices[i]]
            nll = self._compute_nll_at_time_impl(density_est, true_state)
            nll_per_time.append(nll)
        
        nll_per_time = jnp.array(nll_per_time)
        nll_total = float(jnp.sum(nll_per_time))
        
        # 2. è®¡ç®—æ¯æ—¶åˆ»çš„è¦†ç›–ç‡ / Compute per-time coverage
        coverage_per_time = []
        for i, density_est in enumerate(density_estimates):
            true_state = true_trajectory[time_indices[i]]
            coverage = self._compute_coverage_at_time_impl(
                density_est, true_state, self.confidence_level
            )
            coverage_per_time.append(coverage)
        
        coverage_per_time = jnp.array(coverage_per_time)
        coverage_95 = float(jnp.mean(coverage_per_time))
        
        # 3. åŒæ¨¡æ€æ˜¾è‘—æ€§æ£€éªŒ / Bimodality significance test
        bimodality_metrics = self._test_bimodality_significance(density_estimates)
        
        # 4. è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°ï¼ˆè¿‘ä¼¼ï¼‰/ Compute effective sample size (approximation)
        ess = self._estimate_effective_sample_size(density_estimates)
        
        return QualityMetrics(
            nll_total=nll_total,
            nll_per_time=nll_per_time,
            coverage_95=coverage_95,
            coverage_per_time=coverage_per_time,
            bimodality_p_value=bimodality_metrics['p_value'],
            bimodality_detected=bimodality_metrics['detected'],
            theta_ks_statistic=bimodality_metrics['ks_statistic'],
            effective_sample_size=ess
        )
    
    @partial(jax.jit, static_argnums=(0,))
    def _compute_nll_at_time_impl(
        self,
        density_est: DensityEstimate,
        true_state: chex.Array  # [Î¸_true, Ï‰_true]
    ) -> float:
        """
        è®¡ç®—å•æ—¶åˆ»çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ / Compute negative log-likelihood at single time
        
        é€šè¿‡åŒçº¿æ€§æ’å€¼è®¡ç®—çœŸå®çŠ¶æ€åœ¨ä¼°è®¡å¯†åº¦ä¸‹çš„æ¦‚ç‡ã€‚
        Compute probability of true state under estimated density via bilinear interpolation.
        """
        theta_true, omega_true = true_state[0], true_state[1]
        
        # å¤„ç†å‘¨æœŸæ€§è¾¹ç•Œ / Handle periodic boundaries
        theta_wrapped = jnp.mod(theta_true + jnp.pi, 2 * jnp.pi) - jnp.pi
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ç½‘æ ¼èŒƒå›´å†… / Check if within grid bounds
        theta_in_bounds = jnp.logical_and(
            theta_wrapped >= self.theta_bounds[0],
            theta_wrapped <= self.theta_bounds[1]
        )
        omega_in_bounds = jnp.logical_and(
            omega_true >= self.omega_bounds[0],
            omega_true <= self.omega_bounds[1]
        )
        
        in_bounds = jnp.logical_and(theta_in_bounds, omega_in_bounds)
        
        # åŒçº¿æ€§æ’å€¼è®¡ç®—å¯†åº¦å€¼ / Bilinear interpolation for density value
        def interpolate_density():
            # æ‰¾åˆ°ç½‘æ ¼ç´¢å¼• / Find grid indices
            theta_idx = (theta_wrapped - self.theta_bounds[0]) / self.dtheta
            omega_idx = (omega_true - self.omega_bounds[0]) / self.domega
            
            # é™åˆ¶åˆ°æœ‰æ•ˆèŒƒå›´ / Clamp to valid range
            theta_idx = jnp.clip(theta_idx, 0, self.grid_resolution[0] - 1.001)
            omega_idx = jnp.clip(omega_idx, 0, self.grid_resolution[1] - 1.001)
            
            # æ•´æ•°éƒ¨åˆ†å’Œå°æ•°éƒ¨åˆ† / Integer and fractional parts
            i0, i1 = jnp.floor(theta_idx).astype(int), jnp.ceil(theta_idx).astype(int)
            j0, j1 = jnp.floor(omega_idx).astype(int), jnp.ceil(omega_idx).astype(int)
            
            # æ’å€¼æƒé‡ / Interpolation weights
            w_theta = theta_idx - i0
            w_omega = omega_idx - j0
            
            # åŒçº¿æ€§æ’å€¼ / Bilinear interpolation
            density_val = (
                (1 - w_theta) * (1 - w_omega) * density_est.density_2d[i0, j0] +
                w_theta * (1 - w_omega) * density_est.density_2d[i1, j0] +
                (1 - w_theta) * w_omega * density_est.density_2d[i0, j1] +
                w_theta * w_omega * density_est.density_2d[i1, j1]
            )
            
            return density_val
        
        # å¦‚æœåœ¨è¾¹ç•Œå†…åˆ™æ’å€¼ï¼Œå¦åˆ™è¿”å›æå°å€¼ / Interpolate if in bounds, else return tiny value
        density_value = jax.lax.cond(
            in_bounds,
            interpolate_density,
            lambda: 1e-12
        )
        
        # ç¡®ä¿å¯†åº¦ä¸ºæ­£ / Ensure positive density
        density_value = jnp.maximum(density_value, 1e-12)
        
        # è¿”å›è´Ÿå¯¹æ•°ä¼¼ç„¶ / Return negative log-likelihood
        return -jnp.log(density_value)
    
    @partial(jax.jit, static_argnums=(0,))
    def _compute_coverage_at_time_impl(
        self,
        density_est: DensityEstimate,
        true_state: chex.Array,
        confidence_level: float
    ) -> float:
        """
        è®¡ç®—å•æ—¶åˆ»çš„è¦†ç›–ç‡ / Compute coverage at single time
        
        é€šè¿‡éå‚æ•°å¯†åº¦ç§¯åˆ†è®¡ç®—ç½®ä¿¡åŒºé—´è¦†ç›–ã€‚
        Compute confidence interval coverage via non-parametric density integration.
        """
        density_2d = density_est.density_2d
        
        # å±•å¹³å¯†åº¦å¹¶æ’åº / Flatten and sort density
        flat_density = density_2d.flatten()
        sorted_density = jnp.sort(flat_density)[::-1]  # é™åº / descending
        
        # ç´¯ç§¯æ¦‚ç‡è´¨é‡ / Cumulative probability mass
        grid_area = self.dtheta * self.domega
        cumulative_mass = jnp.cumsum(sorted_density) * grid_area
        
        # æ‰¾åˆ°ç½®ä¿¡åŒºé—´é˜ˆå€¼ / Find confidence interval threshold
        threshold_idx = jnp.searchsorted(cumulative_mass, confidence_level)
        threshold_idx = jnp.clip(threshold_idx, 0, len(sorted_density) - 1)
        density_threshold = sorted_density[threshold_idx]
        
        # è®¡ç®—çœŸå®çŠ¶æ€çš„å¯†åº¦å€¼ / Compute density at true state
        true_density = self._interpolate_density_value(density_est, true_state)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ç½®ä¿¡åŒºé—´å†… / Check if within confidence interval
        is_covered = true_density >= density_threshold
        
        return jnp.where(is_covered, 1.0, 0.0)
    
    def _interpolate_density_value(
        self, 
        density_est: DensityEstimate,
        state: chex.Array
    ) -> float:
        """æ’å€¼è®¡ç®—çŠ¶æ€ç‚¹çš„å¯†åº¦å€¼ / Interpolate density value at state point"""
        # é‡å¤NLLè®¡ç®—ä¸­çš„æ’å€¼é€»è¾‘ï¼Œä½†è¿”å›åŸå§‹å¯†åº¦å€¼
        # Repeat interpolation logic from NLL computation but return raw density value
        theta, omega = state[0], state[1]
        theta_wrapped = jnp.mod(theta + jnp.pi, 2 * jnp.pi) - jnp.pi
        
        # æ£€æŸ¥è¾¹ç•Œ / Check bounds
        theta_in_bounds = jnp.logical_and(
            theta_wrapped >= self.theta_bounds[0],
            theta_wrapped <= self.theta_bounds[1]
        )
        omega_in_bounds = jnp.logical_and(
            omega >= self.omega_bounds[0],
            omega <= self.omega_bounds[1]
        )
        
        in_bounds = jnp.logical_and(theta_in_bounds, omega_in_bounds)
        
        def interpolate():
            theta_idx = (theta_wrapped - self.theta_bounds[0]) / self.dtheta
            omega_idx = (omega - self.omega_bounds[0]) / self.domega
            
            theta_idx = jnp.clip(theta_idx, 0, self.grid_resolution[0] - 1.001)
            omega_idx = jnp.clip(omega_idx, 0, self.grid_resolution[1] - 1.001)
            
            i0, i1 = jnp.floor(theta_idx).astype(int), jnp.ceil(theta_idx).astype(int)
            j0, j1 = jnp.floor(omega_idx).astype(int), jnp.ceil(omega_idx).astype(int)
            
            w_theta = theta_idx - i0
            w_omega = omega_idx - j0
            
            return (
                (1 - w_theta) * (1 - w_omega) * density_est.density_2d[i0, j0] +
                w_theta * (1 - w_omega) * density_est.density_2d[i1, j0] +
                (1 - w_theta) * w_omega * density_est.density_2d[i0, j1] +
                w_theta * w_omega * density_est.density_2d[i1, j1]
            )
        
        return jax.lax.cond(in_bounds, interpolate, lambda: 1e-12)
    
    def _test_bimodality_significance(
        self,
        density_estimates: List[DensityEstimate]
    ) -> Dict[str, float]:
        """
        æ£€éªŒåŒæ¨¡æ€æ˜¾è‘—æ€§ / Test bimodality significance
        
        ä½¿ç”¨Kolmogorov-Smirnovæ£€éªŒå’ŒDipæ£€éªŒè¯„ä¼°å¤šæ¨¡æ€æ€§ã€‚
        Use Kolmogorov-Smirnov and Dip tests to assess multi-modality.
        
        Returns:
            metrics: åŒæ¨¡æ€æ£€éªŒç»“æœ / bimodality test results
        """
        # é€‰æ‹©ä¸­é—´æ—¶åˆ»è¿›è¡Œåˆ†æï¼ˆé¢„æœŸæœ€å¤šæ¨¡æ€çš„æ—¶åˆ»ï¼‰
        # Select middle time points for analysis (expected most multi-modal moments)
        mid_idx = len(density_estimates) // 2
        density_est = density_estimates[mid_idx]
        
        # æå–Î¸è¾¹é™…åˆ†å¸ƒ / Extract Î¸ marginal distribution
        theta_marginal = np.array(density_est.marginal_theta)
        theta_grid = np.array(density_est.theta_grid)
        
        # å½’ä¸€åŒ–è¾¹é™…åˆ†å¸ƒ / Normalize marginal distribution
        theta_marginal = theta_marginal / (np.trapz(theta_marginal, theta_grid) + 1e-12)
        
        # ç”Ÿæˆä»è¾¹é™…åˆ†å¸ƒé‡‡æ ·çš„è¿‘ä¼¼æ ·æœ¬ / Generate approximate samples from marginal
        # è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼æ–¹æ³•ï¼Œç”¨äºç»Ÿè®¡æ£€éªŒ / This is an approximation for statistical testing
        n_samples = 1000
        cumulative = np.cumsum(theta_marginal) * (theta_grid[1] - theta_grid[0])
        cumulative = cumulative / cumulative[-1]
        
        # é€†å˜æ¢é‡‡æ · / Inverse transform sampling
        uniform_samples = np.random.uniform(0, 1, n_samples)
        theta_samples = np.interp(uniform_samples, cumulative, theta_grid)
        
        # KSæ£€éªŒå¯¹æ¯”å•æ¨¡æ€ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰/ KS test against unimodal (normal distribution)
        # æ‹Ÿåˆæ­£æ€åˆ†å¸ƒ / Fit normal distribution
        mu_hat = np.mean(theta_samples)
        sigma_hat = np.std(theta_samples)
        
        # KSæ£€éªŒ / KS test
        ks_stat, ks_p_value = stats.kstest(
            theta_samples, 
            lambda x: stats.norm.cdf(x, mu_hat, sigma_hat)
        )
        
        # Dipæ£€éªŒï¼ˆå¦‚æœå¯ç”¨ï¼‰/ Dip test (if available)
        try:
            from diptest import diptest
            dip_stat, dip_p_value = diptest(theta_samples)
            # åˆå¹¶æ£€éªŒç»“æœ / Combine test results
            combined_p_value = min(ks_p_value, dip_p_value)
        except ImportError:
            # ä»…ä½¿ç”¨KSæ£€éªŒ / Use only KS test
            combined_p_value = ks_p_value
            print("Warning: diptest not available, using only KS test")
        
        # æ˜¾è‘—æ€§é˜ˆå€¼ / Significance threshold
        significance_level = 0.05
        bimodality_detected = combined_p_value < significance_level
        
        return {
            'p_value': float(combined_p_value),
            'detected': bool(bimodality_detected),
            'ks_statistic': float(ks_stat)
        }
    
    def _estimate_effective_sample_size(
        self,
        density_estimates: List[DensityEstimate]
    ) -> float:
        """
        ä¼°è®¡æœ‰æ•ˆæ ·æœ¬æ•° / Estimate effective sample size
        
        åŸºäºå¯†åº¦çš„å¹³æ»‘åº¦å’Œå°–é”åº¦ä¼°è®¡ç­‰æ•ˆçš„è’™ç‰¹å¡æ´›æ ·æœ¬æ•°ã€‚
        Estimate equivalent Monte Carlo sample count based on density smoothness and sharpness.
        """
        if not density_estimates:
            return 0.0
        
        # é€‰æ‹©å…¸å‹æ—¶åˆ» / Select representative time point
        mid_density = density_estimates[len(density_estimates) // 2]
        density_2d = np.array(mid_density.density_2d)
        
        # è®¡ç®—ä¿¡æ¯ç†µä½œä¸ºæœ‰æ•ˆæ€§åº¦é‡ / Compute information entropy as effectiveness measure
        density_flat = density_2d.flatten()
        density_normalized = density_flat / (np.sum(density_flat) + 1e-12)
        
        # Shannonç†µ / Shannon entropy
        entropy = -np.sum(
            density_normalized * np.log(density_normalized + 1e-12)
        )
        
        # æœ€å¤§å¯èƒ½ç†µï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰/ Maximum possible entropy (uniform distribution)
        max_entropy = np.log(len(density_flat))
        
        # å½’ä¸€åŒ–ç†µä½œä¸ºæ•ˆç‡æŒ‡æ ‡ / Normalized entropy as efficiency measure
        efficiency = entropy / max_entropy
        
        # ä¼°è®¡ç­‰æ•ˆæ ·æœ¬æ•° / Estimate equivalent sample count
        # è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼ä¼°è®¡ / This is a heuristic estimate
        base_samples = 1000  # åŸºç¡€å‚è€ƒæ ·æœ¬æ•° / base reference sample count
        effective_samples = base_samples * efficiency
        
        return float(effective_samples)
    
    def compare_methods(
        self,
        mmsb_metrics: QualityMetrics,
        baseline_metrics: Dict[str, QualityMetrics],
        method_names: List[str]
    ) -> Dict[str, Dict[str, float]]:
        """
        æ¯”è¾ƒä¸åŒæ–¹æ³•çš„æ€§èƒ½ / Compare performance of different methods
        
        Args:
            mmsb_metrics: MMSB-VIæŒ‡æ ‡ / MMSB-VI metrics
            baseline_metrics: åŸºçº¿æ–¹æ³•æŒ‡æ ‡å­—å…¸ / baseline method metrics dict
            method_names: æ–¹æ³•åç§°åˆ—è¡¨ / method name list
            
        Returns:
            comparison: æ¯”è¾ƒç»“æœ / comparison results
        """
        comparison = {}
        
        # MMSB-VIç»“æœ / MMSB-VI results
        comparison['MMSB-VI'] = {
            'NLL': mmsb_metrics.nll_total,
            'Coverage_95': mmsb_metrics.coverage_95,
            'Bimodality_P': mmsb_metrics.bimodality_p_value,
            'Bimodality_Detected': float(mmsb_metrics.bimodality_detected),
            'Effective_Sample_Size': mmsb_metrics.effective_sample_size
        }
        
        # åŸºçº¿æ–¹æ³•ç»“æœ / Baseline method results
        for method_name in method_names:
            if method_name in baseline_metrics:
                metrics = baseline_metrics[method_name]
                comparison[method_name] = {
                    'NLL': metrics.nll_total,
                    'Coverage_95': metrics.coverage_95,
                    'Bimodality_P': metrics.bimodality_p_value,
                    'Bimodality_Detected': float(metrics.bimodality_detected),
                    'Effective_Sample_Size': metrics.effective_sample_size
                }
        
        # è®¡ç®—ç›¸å¯¹æ”¹è¿› / Compute relative improvements
        for method_name in method_names:
            if method_name in comparison:
                baseline = comparison[method_name]
                mmsb = comparison['MMSB-VI']
                
                # NLLæ”¹è¿›ï¼ˆè¶Šå°è¶Šå¥½ï¼‰/ NLL improvement (lower is better)
                nll_improvement = (baseline['NLL'] - mmsb['NLL']) / baseline['NLL']
                
                # è¦†ç›–ç‡æ”¹è¿›ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰/ Coverage improvement (higher is better)
                coverage_improvement = (mmsb['Coverage_95'] - baseline['Coverage_95'])
                
                comparison[f'{method_name}_vs_MMSB'] = {
                    'NLL_improvement_pct': nll_improvement * 100,
                    'Coverage_improvement': coverage_improvement,
                    'Bimodality_advantage': mmsb['Bimodality_Detected'] - baseline['Bimodality_Detected']
                }
        
        return comparison
    
    def print_comparison_summary(self, comparison: Dict[str, Dict[str, float]]):
        """æ‰“å°æ¯”è¾ƒç»“æœæ‘˜è¦ / Print comparison summary"""
        print("\n" + "="*70)
        print("æ¦‚ç‡å¯†åº¦è´¨é‡è¯„ä¼°å¯¹æ¯”ç»“æœ / Probability Density Quality Assessment Comparison")
        print("="*70)
        
        # ä¸»è¦æŒ‡æ ‡å¯¹æ¯” / Main metrics comparison
        methods = [k for k in comparison.keys() if not k.endswith('_vs_MMSB')]
        
        print(f"\n{'æ–¹æ³•/Method':<15} {'NLL':<12} {'Coverage':<12} {'Bimodal':<10} {'ESS':<10}")
        print("-" * 65)
        
        for method in methods:
            metrics = comparison[method]
            print(f"{method:<15} {metrics['NLL']:<12.2f} {metrics['Coverage_95']:<12.3f} "
                  f"{metrics['Bimodality_Detected']:<10.0f} {metrics['Effective_Sample_Size']:<10.0f}")
        
        # MMSB-VIä¼˜åŠ¿åˆ†æ / MMSB-VI advantage analysis
        print(f"\nğŸ¯ MMSB-VIç›¸å¯¹æ”¹è¿› / MMSB-VI Relative Improvements:")
        for key in comparison:
            if key.endswith('_vs_MMSB'):
                method_name = key.replace('_vs_MMSB', '')
                improvements = comparison[key]
                print(f"  vs {method_name}:")
                print(f"    NLLæ”¹è¿›: {improvements['NLL_improvement_pct']:.1f}%")
                print(f"    è¦†ç›–ç‡æ”¹è¿›: {improvements['Coverage_improvement']:.3f}")
                print(f"    åŒæ¨¡æ€ä¼˜åŠ¿: {improvements['Bimodality_advantage']:.0f}")


if __name__ == "__main__":
    # æµ‹è¯•å¯†åº¦è´¨é‡è¯„ä¼°å™¨ / Test density quality assessor
    print("ğŸ§ª æµ‹è¯•æ¦‚ç‡å¯†åº¦è´¨é‡è¯„ä¼°æŒ‡æ ‡")
    print("ğŸ§ª Testing Probability Density Quality Assessment Metrics")
    
    # åˆ›å»ºè¯„ä¼°å™¨ / Create assessor
    assessor = DensityQualityMetrics(
        grid_resolution=(32, 16),
        confidence_level=0.95
    )
    
    print("âœ… æ¦‚ç‡å¯†åº¦è´¨é‡è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    print("âœ… Probability density quality assessor initialized")