"""
Flax-based FÃ¶llmer Drift Networks
åŸºäºFlaxçš„FÃ¶llmeræ¼‚ç§»ç½‘ç»œ

High-performance JAX/Flax implementation of neural networks for drift parametrization.
é«˜æ€§èƒ½JAX/Flaxå®ç°çš„æ¼‚ç§»å‚æ•°åŒ–ç¥ç»ç½‘ç»œã€‚
"""

import jax
import jax.numpy as jnp
import jax.random
from jax import lax, vmap, pmap, jit
from jax.nn import silu, gelu, swish
from functools import partial
from typing import Optional, Callable, Tuple, Any, Dict, List
import math

from flax import linen as nn
from flax.linen import LayerNorm, GroupNorm
from flax.training import train_state
import optax
import chex


from ..core.types import (
    SDEState, DriftFunction, NetworkParams, TimeEncoding, BatchStates, BatchTimes,
    NetworkConfig, TrainingConfig, PerformanceConfig, NetworkTrainingState
)
from ..core.registry import register_network


# ============================================================================
# Time Encoding Module / æ—¶é—´ç¼–ç æ¨¡å—
# ============================================================================

class TimeEncoder(nn.Module):
    """
    Sinusoidal positional encoding for time conditioning
    æ—¶é—´æ¡ä»¶çš„æ­£å¼¦ä½ç½®ç¼–ç 
    
    Provides efficient, cache-friendly time encodings with GPU optimization.
    æä¾›é«˜æ•ˆã€ç¼“å­˜å‹å¥½çš„æ—¶é—´ç¼–ç å’ŒGPUä¼˜åŒ–ã€‚
    """
    
    encoding_dim: int = 128
    max_time: float = 100.0
    learnable_scaling: bool = True
    use_cache: bool = True
    
    def setup(self):
        """Initialize time encoding parameters / åˆå§‹åŒ–æ—¶é—´ç¼–ç å‚æ•°"""
        # é¢„è®¡ç®—ç¼–ç é¢‘ç‡ / Precompute encoding frequencies
        half_dim = self.encoding_dim // 2
        freqs = jnp.exp(-math.log(10000.0) * jnp.arange(half_dim) / (half_dim - 1))
        self.freqs = freqs
        
        # å¯å­¦ä¹ çš„ç¼©æ”¾å› å­ / Learnable scaling factors
        if self.learnable_scaling:
            self.time_scale = self.param('time_scale', nn.initializers.ones, (1,))
            self.freq_scale = self.param('freq_scale', nn.initializers.ones, (half_dim,))
        
        # æ—¶é—´ç¼–ç ç¼“å­˜ / Time encoding cache
        if self.use_cache:
            self._cache = {}
    
    def encode_time(self, t: float) -> TimeEncoding:
        """
        Encode single time point
        ç¼–ç å•ä¸ªæ—¶é—´ç‚¹
        
        Args:
            t: Time value / æ—¶é—´å€¼
            
        Returns:
            encoding: Time encoding vector / æ—¶é—´ç¼–ç å‘é‡
        """
        if self.learnable_scaling:
            scaled_t = t * self.time_scale
            scaled_freqs = self.freqs * self.freq_scale
        else:
            scaled_t = t
            scaled_freqs = self.freqs
        
        # è®¡ç®—æ­£å¼¦å’Œä½™å¼¦ç¼–ç  / Compute sine and cosine encodings
        angles = scaled_t * scaled_freqs
        sin_enc = jnp.sin(angles)
        cos_enc = jnp.cos(angles)
        
        # äº¤é”™ç»„åˆ / Interleave
        encoding = jnp.concatenate([sin_enc, cos_enc], axis=-1)
        
        # å¦‚æœç»´åº¦æ˜¯å¥‡æ•°ï¼Œæˆªæ–­æœ€åä¸€ä¸ªå…ƒç´  / Truncate if odd dimension
        if self.encoding_dim % 2 == 1:
            encoding = encoding[:self.encoding_dim]
            
        return encoding
    
    def __call__(self, t: jnp.ndarray) -> jnp.ndarray:
        """
        Vectorized time encoding
        å‘é‡åŒ–æ—¶é—´ç¼–ç 
        
        Args:
            t: Time values (can be scalar or batch) / æ—¶é—´å€¼ï¼ˆæ ‡é‡æˆ–æ‰¹é‡ï¼‰
            
        Returns:
            encodings: Time encodings / æ—¶é—´ç¼–ç 
        """
        # FIX: Robustly handle Python floats by converting to JAX array.
        # This prevents the "'float' object has no attribute 'ndim'" error.
        # ä¿®å¤ï¼šé€šè¿‡è½¬æ¢ä¸ºJAXæ•°ç»„æ¥ç¨³å¥åœ°å¤„ç†Pythonæµ®ç‚¹æ•°ã€‚
        # è¿™å¯ä»¥é˜²æ­¢â€œ'float'å¯¹è±¡æ²¡æœ‰'ndim'å±æ€§â€çš„é”™è¯¯ã€‚
        t = jnp.asarray(t)
        
        # å¦‚æœæ˜¯æ ‡é‡ï¼Œç›´æ¥ç¼–ç  / If scalar, encode directly
        if t.ndim == 0:
            return self.encode_time(t)
        
        # å¦‚æœæ˜¯æ‰¹é‡ï¼Œä½¿ç”¨vmapå¹¶è¡ŒåŒ– / If batch, use vmap for parallelization
        return vmap(self.encode_time)(t)


# ============================================================================
# Time-Conditioned ResNet Block / æ—¶é—´æ¡ä»¶ResNetå—
# ============================================================================

class TimeConditionedResNetBlock(nn.Module):
    """
    ResNet block with time conditioning
    å¸¦æ—¶é—´æ¡ä»¶çš„ResNetå—
    
    Features:
    - Time-dependent feature modulation
    - Spectral normalization for stability
    - Efficient GPU computation
    
    ç‰¹æ€§ï¼š
    - æ—¶é—´ç›¸å…³çš„ç‰¹å¾è°ƒåˆ¶
    - è°±å½’ä¸€åŒ–æé«˜ç¨³å®šæ€§
    - é«˜æ•ˆGPUè®¡ç®—
    """
    
    hidden_dim: int
    activation: str = "silu"
    use_spectral_norm: bool = True
    use_layer_norm: bool = True
    dropout_rate: float = 0.1
    time_conditioning_type: str = "film"  # "film", "concat", "add"
    
    def setup(self):
        """Initialize ResNet block components / åˆå§‹åŒ–ResNetå—ç»„ä»¶"""
        # æ¿€æ´»å‡½æ•°é€‰æ‹© / Activation function selection
        if self.activation == "silu":
            self.act_fn = silu
        elif self.activation == "gelu":
            self.act_fn = gelu
        elif self.activation == "swish":
            self.act_fn = swish
        else:
            raise ValueError(f"Unsupported activation: {self.activation}")
        
        # ä¸»è¦å˜æ¢å±‚ / Main transformation layers
        self.dense1 = nn.Dense(self.hidden_dim, use_bias=False)
        self.dense2 = nn.Dense(self.hidden_dim, use_bias=False)
        
        # æ—¶é—´æ¡ä»¶å±‚ / Time conditioning layers
        if self.time_conditioning_type == "film":
            # Feature-wise Linear Modulation
            self.time_mlp = nn.Sequential([
                nn.Dense(self.hidden_dim * 2),
                self.act_fn,
                nn.Dense(self.hidden_dim * 2)
            ])
        elif self.time_conditioning_type == "concat":
            # é€‚åº”è¿æ¥åçš„ç»´åº¦ / Adapt for concatenated dimension
            self.time_dense = nn.Dense(self.hidden_dim // 4)
            self.combined_dense = nn.Dense(self.hidden_dim)
        elif self.time_conditioning_type == "add":
            # å¦‚æœæ—¶é—´ç¼–ç ç»´åº¦ä¸ç‰¹å¾ç»´åº¦ä¸åŒ¹é…ï¼Œåˆ™éœ€è¦ä¸€ä¸ªæŠ•å½±å±‚
            # If time encoding dimension doesn't match feature dimension, a projection layer is needed.
            # BUG FIX: Layer must be defined in setup(), not __call__().
            # é”™è¯¯ä¿®å¤ï¼šå±‚å¿…é¡»åœ¨setup()ä¸­å®šä¹‰ï¼Œè€Œä¸æ˜¯åœ¨__call__()ä¸­ã€‚
            self.time_proj_add = nn.Dense(self.hidden_dim)
        
        # å½’ä¸€åŒ–å±‚ / Normalization layers
        if self.use_layer_norm:
            self.norm1 = LayerNorm()
            self.norm2 = LayerNorm()
        
        # Dropoutå±‚ / Dropout layers
        if self.dropout_rate > 0:
            self.dropout = nn.Dropout(rate=self.dropout_rate)
    
    def __call__(self, x: jnp.ndarray, time_enc: jnp.ndarray, 
                 train: bool = False, deterministic: bool = None) -> jnp.ndarray:
        """
        Forward pass with time conditioning
        å¸¦æ—¶é—´æ¡ä»¶çš„å‰å‘ä¼ æ’­
        
        Args:
            x: Input features / è¾“å…¥ç‰¹å¾
            time_enc: Time encoding / æ—¶é—´ç¼–ç 
            train: Training mode / è®­ç»ƒæ¨¡å¼
            deterministic: Deterministic mode for dropout / Dropoutç¡®å®šæ€§æ¨¡å¼
            
        Returns:
            output: Transformed features / å˜æ¢åçš„ç‰¹å¾
        """
        if deterministic is None:
            deterministic = not train
        
        residual = x
        
        # ç¬¬ä¸€ä¸ªå˜æ¢ / First transformation
        if self.use_layer_norm:
            x = self.norm1(x)
        x = self.dense1(x)
        
        # æ—¶é—´æ¡ä»¶åº”ç”¨ / Apply time conditioning
        x = self._apply_time_conditioning(x, time_enc)
        
        x = self.act_fn(x)
        
        # Dropout
        if self.dropout_rate > 0 and not deterministic:
            x = self.dropout(x, deterministic=deterministic)
        
        # ç¬¬äºŒä¸ªå˜æ¢ / Second transformation
        if self.use_layer_norm:
            x = self.norm2(x)
        x = self.dense2(x)
        
        # æ®‹å·®è¿æ¥ / Residual connection
        return x + residual
    
    def _apply_time_conditioning(self, x: jnp.ndarray, time_enc: jnp.ndarray) -> jnp.ndarray:
        """
        Apply time conditioning to features
        å¯¹ç‰¹å¾åº”ç”¨æ—¶é—´æ¡ä»¶
        """
        if self.time_conditioning_type == "film":
            # Feature-wise Linear Modulation
            time_params = self.time_mlp(time_enc)
            gamma, beta = jnp.split(time_params, 2, axis=-1)
            
            # å¹¿æ’­åˆ°batchç»´åº¦ / Broadcast to batch dimension
            if x.ndim > time_enc.ndim:
                gamma = jnp.expand_dims(gamma, axis=0)
                beta = jnp.expand_dims(beta, axis=0)
            
            return gamma * x + beta
            
        elif self.time_conditioning_type == "concat":
            # è¿æ¥æ—¶é—´ç¼–ç  / Concatenate time encoding
            time_feat = self.time_dense(time_enc)
            if x.ndim > time_feat.ndim:
                time_feat = jnp.expand_dims(time_feat, axis=0)
                time_feat = jnp.broadcast_to(time_feat, (x.shape[0], time_feat.shape[-1]))
            
            combined = jnp.concatenate([x, time_feat], axis=-1)
            return self.combined_dense(combined)
            
        elif self.time_conditioning_type == "add":
            # ç®€å•ç›¸åŠ ï¼ˆéœ€è¦ç»´åº¦åŒ¹é…ï¼‰/ Simple addition (requires dimension matching)
            # ä½¿ç”¨åœ¨setup()ä¸­å®šä¹‰çš„æŠ•å½±å±‚
            # Use the projection layer defined in setup()
            if time_enc.shape[-1] != x.shape[-1]:
                time_proj = self.time_proj_add(time_enc)
            else:
                time_proj = time_enc
            
            if x.ndim > time_proj.ndim:
                time_proj = jnp.expand_dims(time_proj, axis=0)
            
            return x + time_proj
        
        else:
            return x


# ============================================================================
# Self-Attention Block / è‡ªæ³¨æ„åŠ›å—
# ============================================================================

class SelfAttentionBlock(nn.Module):
    """
    Self-attention block with time-dependent weights
    å¸¦æ—¶é—´ç›¸å…³æƒé‡çš„è‡ªæ³¨æ„åŠ›å—
    
    Optimized for GPU parallel computation with Flash Attention patterns.
    ä¼˜åŒ–GPUå¹¶è¡Œè®¡ç®—ï¼Œé‡‡ç”¨Flash Attentionæ¨¡å¼ã€‚
    """
    
    num_heads: int = 8
    head_dim: int = 64
    use_time_conditioning: bool = True
    dropout_rate: float = 0.1
    
    def setup(self):
        """Initialize attention components / åˆå§‹åŒ–æ³¨æ„åŠ›ç»„ä»¶"""
        self.embed_dim = self.num_heads * self.head_dim
        
        # Q, K, VæŠ•å½± / Q, K, V projections
        self.q_proj = nn.Dense(self.embed_dim, use_bias=False)
        self.k_proj = nn.Dense(self.embed_dim, use_bias=False)  
        self.v_proj = nn.Dense(self.embed_dim, use_bias=False)
        
        # è¾“å‡ºæŠ•å½± / Output projection
        self.out_proj = nn.Dense(self.embed_dim)
        
        # æ—¶é—´æ¡ä»¶æƒé‡ / Time conditioning weights
        if self.use_time_conditioning:
            self.time_proj = nn.Dense(self.num_heads)
        
        # Dropout
        if self.dropout_rate > 0:
            self.dropout = nn.Dropout(rate=self.dropout_rate)
        
        # ç¼©æ”¾å› å­ / Scaling factor
        self.scale = 1.0 / math.sqrt(self.head_dim)
    
    def __call__(self, x: jnp.ndarray, time_enc: Optional[jnp.ndarray] = None,
                 train: bool = False, deterministic: bool = None) -> jnp.ndarray:
        """
        Self-attention forward pass
        è‡ªæ³¨æ„åŠ›å‰å‘ä¼ æ’­
        
        Args:
            x: Input features [batch_size, seq_len, embed_dim] / è¾“å…¥ç‰¹å¾
            time_enc: Time encoding / æ—¶é—´ç¼–ç 
            train: Training mode / è®­ç»ƒæ¨¡å¼
            deterministic: Deterministic mode / ç¡®å®šæ€§æ¨¡å¼
            
        Returns:
            output: Attention output / æ³¨æ„åŠ›è¾“å‡º
        """
        if deterministic is None:
            deterministic = not train
        
        batch_size, seq_len, _ = x.shape
        
        # è®¡ç®—Q, K, V / Compute Q, K, V
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ / Reshape for multi-head
        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # è½¬ç½®ä¸º [batch, num_heads, seq_len, head_dim] / Transpose to [batch, num_heads, seq_len, head_dim]
        q = jnp.transpose(q, (0, 2, 1, 3))
        k = jnp.transpose(k, (0, 2, 1, 3))
        v = jnp.transpose(v, (0, 2, 1, 3))
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° / Compute attention scores
        scores = jnp.einsum('bhqd,bhkd->bhqk', q, k) * self.scale
        
        # æ—¶é—´æ¡ä»¶æƒé‡ / Time conditioning weights
        if self.use_time_conditioning and time_enc is not None:
            time_weights = self.time_proj(time_enc)  # [time_enc_dim] -> [num_heads] or [batch, num_heads]
            time_weights = jax.nn.softmax(time_weights)
            # è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…scores / Adjust shape to match scores
            if time_weights.ndim == 1:
                # å•æ ·æœ¬æƒ…å†µ / Single sample case
                time_weights = time_weights.reshape(1, self.num_heads, 1, 1)
            else:
                # æ‰¹é‡æƒ…å†µ / Batch case
                time_weights = time_weights.reshape(batch_size, self.num_heads, 1, 1)
            scores = scores * time_weights
        
        # Softmaxæ³¨æ„åŠ›æƒé‡ / Softmax attention weights
        attn_weights = jax.nn.softmax(scores, axis=-1)
        
        # Dropout
        if self.dropout_rate > 0 and not deterministic:
            attn_weights = self.dropout(attn_weights, deterministic=deterministic)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡ / Apply attention weights
        attn_output = jnp.einsum('bhqk,bhkd->bhqd', attn_weights, v)
        
        # é‡æ–°ç»„åˆå¤´ / Recombine heads
        attn_output = jnp.transpose(attn_output, (0, 2, 1, 3))
        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)
        
        # è¾“å‡ºæŠ•å½± / Output projection
        output = self.out_proj(attn_output)
        
        return output


# ============================================================================
# Main FÃ¶llmer Drift Network / ä¸»è¦FÃ¶llmeræ¼‚ç§»ç½‘ç»œ
# ============================================================================

@register_network("follmer_drift")
class FÃ¶llmerDriftNet(nn.Module):
    """
    High-performance FÃ¶llmer drift neural network
    é«˜æ€§èƒ½FÃ¶llmeræ¼‚ç§»ç¥ç»ç½‘ç»œ
    
    Architecture:
    - Time-conditioned input embedding
    - Multiple ResNet blocks with time conditioning
    - Self-attention for long-range dependencies
    - Output projection with orthogonal initialization
    
    æ¶æ„ï¼š
    - æ—¶é—´æ¡ä»¶è¾“å…¥åµŒå…¥
    - å¤šä¸ªå¸¦æ—¶é—´æ¡ä»¶çš„ResNetå—
    - è‡ªæ³¨æ„åŠ›æ•è·é•¿è·ç¦»ä¾èµ–
    - æ­£äº¤åˆå§‹åŒ–çš„è¾“å‡ºæŠ•å½±
    
    GPU Optimizations:
    - JIT compilation with static arguments
    - Efficient vectorization patterns
    - Memory-efficient attention
    - Mixed precision support
    """
    
    config: NetworkConfig
    state_dim: int
    
    def setup(self):
        """Initialize network components / åˆå§‹åŒ–ç½‘ç»œç»„ä»¶"""
        # æ—¶é—´ç¼–ç å™¨ / Time encoder
        self.time_encoder = TimeEncoder(
            encoding_dim=self.config.time_encoding_dim,
            learnable_scaling=True,
            use_cache=True
        )
        
        # è¾“å…¥åµŒå…¥ / Input embedding
        self.input_embedding = nn.Dense(
            self.config.hidden_dims[0], 
            use_bias=True
        )
        
        # æ—¶é—´åµŒå…¥ / Time embedding
        self.time_embedding = nn.Dense(
            self.config.hidden_dims[0],
            use_bias=True
        )
        
        # ResNetå—åºåˆ— / ResNet block sequence
        for i in range(self.config.n_layers):
            hidden_dim = self.config.hidden_dims[min(i, len(self.config.hidden_dims) - 1)]
            block = TimeConditionedResNetBlock(
                hidden_dim=hidden_dim,
                activation=self.config.activation,
                use_spectral_norm=self.config.use_spectral_norm,
                use_layer_norm=self.config.use_layer_norm,
                dropout_rate=self.config.dropout_rate
            )
            setattr(self, f'resnet_block_{i}', block)
        
        # è‡ªæ³¨æ„åŠ›å— / Self-attention block
        if self.config.use_attention:
            self.attention = SelfAttentionBlock(
                num_heads=8,
                head_dim=self.config.hidden_dims[0] // 8,
                use_time_conditioning=True,
                dropout_rate=self.config.dropout_rate
            )
        
        # æœ€ç»ˆè¾“å‡ºå±‚ï¼ˆå°æ–¹å·®åˆå§‹åŒ–æé«˜æ¢¯åº¦ç¨³å®šæ€§ï¼‰ / Final output layer (small variance init for gradient stability)
        self.output_layer = nn.Dense(
            self.state_dim,
            kernel_init=nn.initializers.normal(stddev=0.01),  # å°æ–¹å·®åˆå§‹åŒ–é˜²æ¢¯åº¦çˆ†ç‚¸ / Small variance init to prevent gradient explosion
            bias_init=nn.initializers.zeros
        )
        
        # è¾“å‡ºç¼©æ”¾ / Output scaling
        self.output_scale = self.param(
            'output_scale', 
            nn.initializers.ones, 
            (1,)
        )
    
    def __call__(self, x: jnp.ndarray, t: jnp.ndarray, 
                 train: bool = False, deterministic: bool = None) -> jnp.ndarray:
        """
        Forward pass
        å‰å‘ä¼ æ’­
        
        Args:
            x: State vector(s) / çŠ¶æ€å‘é‡
            t: Time value(s) / æ—¶é—´å€¼
            train: Training mode / è®­ç»ƒæ¨¡å¼
            deterministic: Deterministic mode / ç¡®å®šæ€§æ¨¡å¼
            
        Returns:
            drift: Predicted drift Î¼(x,t) / é¢„æµ‹æ¼‚ç§»
        """
        if deterministic is None:
            deterministic = not train
        
        # æ—¶é—´ç¼–ç  / Time encoding
        time_enc = self.time_encoder(t)
        
        # è¾“å…¥å¤„ç† / Input processing
        batch_processing = x.ndim > 1
        if not batch_processing:
            x = jnp.expand_dims(x, axis=0)
            time_enc = jnp.expand_dims(time_enc, axis=0)
        
        # è¾“å…¥åµŒå…¥ / Input embedding
        h = self.input_embedding(x)
        
        # æ—¶é—´åµŒå…¥å¹¶èåˆ / Time embedding and fusion
        time_emb = self.time_embedding(time_enc)
        h = h + time_emb
        
        # ResNetå—å¤„ç† / ResNet block processing
        for i in range(self.config.n_layers):
            block = getattr(self, f'resnet_block_{i}')
            h = block(h, time_enc, train=train, deterministic=deterministic)
        
        # è‡ªæ³¨æ„åŠ›å¤„ç† / Self-attention processing
        if self.config.use_attention:
            # BUG FIX: The self-attention block is mathematically degenerate for a sequence of length 1.
            # Applying softmax to a single logit always results in a weight of 1,
            # which nullifies the gradients for the query and key projection networks (q_proj, k_proj).
            # This block should only be used for genuine sequences (seq_len > 1).
            # Since the input is always a single time-step state, we bypass this block.
            #
            # é”™è¯¯ä¿®å¤ï¼šè‡ªæ³¨æ„åŠ›å—å¯¹äºé•¿åº¦ä¸º1çš„åºåˆ—åœ¨æ•°å­¦ä¸Šæ˜¯ç®€å¹¶çš„ã€‚
            # å¯¹å•ä¸ªlogitåº”ç”¨softmaxæ€»æ˜¯å¾—åˆ°æƒé‡1ï¼Œè¿™ä¼šä½¿æŸ¥è¯¢å’Œé”®æŠ•å½±ç½‘ç»œ(q_proj, k_proj)çš„æ¢¯åº¦ä¸ºé›¶ã€‚
            # æ­¤å—åªåº”ç”¨äºçœŸå®çš„åºåˆ—(seq_len > 1)ã€‚
            # ç”±äºè¾“å…¥å§‹ç»ˆæ˜¯å•ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€ï¼Œæˆ‘ä»¬ç»•è¿‡æ­¤å—ã€‚
            #
            # åŸå§‹é—®é¢˜ä»£ç  (Original problematic code):
            # h_seq = jnp.expand_dims(h, axis=1)  # [batch, 1, hidden]
            # h_attn = self.attention(h_seq, time_enc, train=train, deterministic=deterministic)
            # h = jnp.squeeze(h_attn, axis=1)  # [batch, hidden]
            pass  # Bypassing the attention block.
        
        # æœ€ç»ˆè¾“å‡ºï¼ˆæ·»åŠ è£å‰ªé˜²æ¢¯åº¦çˆ†ç‚¸ï¼‰ / Final output (with clipping to prevent gradient explosion)
        drift = self.output_layer(h)
        drift = drift * self.output_scale
        
        # è¾“å‡ºè£å‰ªæé«˜æ•°å€¼ç¨³å®šæ€§ / Output clipping for numerical stability
        drift = jnp.clip(drift, -5.0, 5.0)  # é™åˆ¶driftè¾“å‡ºèŒƒå›´é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ / Limit drift output range to prevent gradient explosion
        
        # ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰/ Remove batch dimension if needed
        if not batch_processing:
            drift = jnp.squeeze(drift, axis=0)
        
        return drift
    
    def batch_call(self, x_batch: BatchStates, t_batch: BatchTimes, 
                   train: bool = False) -> BatchStates:
        """
        Efficient batch processing
        é«˜æ•ˆæ‰¹é‡å¤„ç†
        
        Args:
            x_batch: Batch of states [batch_size, state_dim] / çŠ¶æ€æ‰¹é‡
            t_batch: Batch of times [batch_size] / æ—¶é—´æ‰¹é‡
            train: Training mode / è®­ç»ƒæ¨¡å¼
            
        Returns:
            drift_batch: Batch of drifts / æ¼‚ç§»æ‰¹é‡
        """
        return self.__call__(x_batch, t_batch, train=train)


# ============================================================================
# Multi-Scale FÃ¶llmer Drift Network / å¤šå°ºåº¦FÃ¶llmeræ¼‚ç§»ç½‘ç»œ
# ============================================================================

@register_network("multiscale_follmer_drift")
class MultiScaleFÃ¶llmerDrift(nn.Module):
    """
    Multi-scale FÃ¶llmer drift network for complex dynamics
    å¤æ‚åŠ¨åŠ›å­¦çš„å¤šå°ºåº¦FÃ¶llmeræ¼‚ç§»ç½‘ç»œ
    
    Features multiple time scales and hierarchical processing.
    ç‰¹æ€§åŒ…æ‹¬å¤šæ—¶é—´å°ºåº¦å’Œåˆ†å±‚å¤„ç†ã€‚
    """
    
    config: NetworkConfig
    state_dim: int
    num_scales: int = 3
    
    def setup(self):
        """Initialize multi-scale components / åˆå§‹åŒ–å¤šå°ºåº¦ç»„ä»¶"""
        # å¤šå°ºåº¦ç½‘ç»œ / Multi-scale networks
        for scale in range(self.num_scales):
            # æ¯ä¸ªå°ºåº¦æœ‰ä¸åŒçš„é…ç½® / Each scale has different configuration
            scale_config = self.config.replace(
                hidden_dims=tuple(dim // (scale + 1) for dim in self.config.hidden_dims),
                n_layers=max(2, self.config.n_layers - scale)
            )
            
            network = FÃ¶llmerDriftNet(
                config=scale_config,
                state_dim=self.state_dim
            )
            setattr(self, f'scale_network_{scale}', network)
        
        # å°ºåº¦èåˆå±‚ / Scale fusion layer
        self.scale_fusion = nn.Dense(
            self.state_dim,
            kernel_init=nn.initializers.he_normal()
        )
        
        # å°ºåº¦æƒé‡ / Scale weights
        self.scale_weights = self.param(
            'scale_weights',
            nn.initializers.uniform(scale=0.1),
            (self.num_scales,)
        )
    
    def __call__(self, x: jnp.ndarray, t: jnp.ndarray,
                 train: bool = False, deterministic: bool = None) -> jnp.ndarray:
        """
        Multi-scale forward pass
        å¤šå°ºåº¦å‰å‘ä¼ æ’­
        """
        if deterministic is None:
            deterministic = not train
        
        # è®¡ç®—æ¯ä¸ªå°ºåº¦çš„è¾“å‡º / Compute output for each scale
        scale_outputs = []
        for i in range(self.num_scales):
            network = getattr(self, f'scale_network_{i}')
            # ä¸åŒå°ºåº¦ä½¿ç”¨ä¸åŒçš„æ—¶é—´ç¼©æ”¾ / Different time scaling for each scale
            scale_factor = 2.0 ** i
            scaled_t = t / scale_factor
            
            output = network(x, scaled_t, train=train, deterministic=deterministic)
            scale_outputs.append(output)
        
        # åŠ æƒèåˆå¤šå°ºåº¦è¾“å‡º / Weighted fusion of multi-scale outputs
        weights = jax.nn.softmax(self.scale_weights)
        
        fused_output = jnp.zeros_like(scale_outputs[0])
        for i, (output, weight) in enumerate(zip(scale_outputs, weights)):
            fused_output += weight * output
        
        # æœ€ç»ˆèåˆå¤„ç† / Final fusion processing
        final_drift = self.scale_fusion(fused_output)
        
        return final_drift


# ============================================================================
# GPU Parallel Computing Utilities / GPUå¹¶è¡Œè®¡ç®—å·¥å…·
# ============================================================================

def create_parallel_drift_function(
    network: nn.Module,
    performance_config: PerformanceConfig
) -> Callable:
    """
    Create GPU-optimized parallel drift function
    åˆ›å»ºGPUä¼˜åŒ–çš„å¹¶è¡Œæ¼‚ç§»å‡½æ•°
    
    Args:
        network: Drift network / æ¼‚ç§»ç½‘ç»œ
        performance_config: Performance configuration / æ€§èƒ½é…ç½®
        
    Returns:
        parallel_drift_fn: Optimized parallel function / ä¼˜åŒ–çš„å¹¶è¡Œå‡½æ•°
    """
    
    @partial(jit, static_argnums=(0, 4))
    def single_drift_fn(params: NetworkParams, x: SDEState, t: float, 
                       rngs: Dict[str, jax.random.PRNGKey], train: bool = False) -> SDEState:
        """Single sample drift computation / å•æ ·æœ¬æ¼‚ç§»è®¡ç®—"""
        return network.apply(params, x, t, train=train, rngs=rngs)
    
    if performance_config.use_vmap:
        # å‘é‡åŒ–æ‰¹é‡å¤„ç† / Vectorized batch processing
        @partial(jit, static_argnums=(0, 4))
        def batch_drift_fn(params: NetworkParams, x_batch: BatchStates, 
                          t_batch: BatchTimes, rngs: Dict[str, jax.random.PRNGKey],
                          train: bool = False) -> BatchStates:
            """Vectorized batch drift computation / å‘é‡åŒ–æ‰¹é‡æ¼‚ç§»è®¡ç®—"""
            
            # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…éšæœºæ•°ç”Ÿæˆå™¨ / Allocate RNG for each sample
            if rngs and 'dropout' in rngs:
                batch_size = x_batch.shape[0]
                dropout_keys = jax.random.split(rngs['dropout'], batch_size)
                batch_rngs = {'dropout': dropout_keys}
            else:
                batch_rngs = rngs
            
            # ä½¿ç”¨vmapå¹¶è¡ŒåŒ– / Parallelize with vmap
            vmap_fn = vmap(
                single_drift_fn,
                in_axes=(None, 0, 0, 0, None)  # params shared, others batched
            )
            
            return vmap_fn(params, x_batch, t_batch, batch_rngs, train)
        
        parallel_fn = batch_drift_fn
    else:
        parallel_fn = single_drift_fn
    
    if performance_config.use_pmap and performance_config.num_devices > 1:
        # å¤šè®¾å¤‡å¹¶è¡Œå¤„ç† / Multi-device parallel processing
        @partial(pmap, static_broadcasted_argnums=(4,))
        def multi_device_drift_fn(params: NetworkParams, x_shards: BatchStates,
                                 t_shards: BatchTimes, rngs_shards: Dict[str, jax.random.PRNGKey],
                                 train: bool = False) -> BatchStates:
            """Multi-device parallel drift computation / å¤šè®¾å¤‡å¹¶è¡Œæ¼‚ç§»è®¡ç®—"""
            return parallel_fn(params, x_shards, t_shards, rngs_shards, train)
        
        parallel_fn = multi_device_drift_fn
    
    return parallel_fn


def create_time_efficient_integrator(
    drift_fn: Callable,
    performance_config: PerformanceConfig
) -> Callable:
    """
    Create memory-efficient time integration using scan
    ä½¿ç”¨scanåˆ›å»ºå†…å­˜é«˜æ•ˆçš„æ—¶é—´ç§¯åˆ†
    
    Args:
        drift_fn: Drift function / æ¼‚ç§»å‡½æ•°
        performance_config: Performance configuration / æ€§èƒ½é…ç½®
        
    Returns:
        integrator_fn: Efficient integrator / é«˜æ•ˆç§¯åˆ†å™¨
    """
    
    def scan_step(carry, scan_input):
        """Single integration step / å•ä¸ªç§¯åˆ†æ­¥éª¤"""
        x_curr, params, rngs = carry
        t_curr, dt, noise_key = scan_input
        
        # è®¡ç®—æ¼‚ç§» / Compute drift
        drift = drift_fn(params, x_curr, t_curr, rngs, train=False)
        
        # SDEæ­¥éª¤æ›´æ–° / SDE step update
        noise = jax.random.normal(noise_key, x_curr.shape)
        x_next = x_curr + drift * dt + jnp.sqrt(dt) * noise
        
        return (x_next, params, rngs), x_next
    
    if performance_config.use_scan:
        @partial(jit, static_argnums=(0,))
        def efficient_integrate(params: NetworkParams, x0: SDEState,
                               time_grid: jnp.ndarray, key: jax.random.PRNGKey,
                               rngs: Dict[str, jax.random.PRNGKey]) -> jnp.ndarray:
            """Efficient time integration / é«˜æ•ˆæ—¶é—´ç§¯åˆ†"""
            
            n_steps = len(time_grid) - 1
            dt_values = jnp.diff(time_grid)
            noise_keys = jax.random.split(key, n_steps)
            
            scan_inputs = (time_grid[:-1], dt_values, noise_keys)
            init_carry = (x0, params, rngs)
            
            if performance_config.use_checkpointing:
                # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜ / Use gradient checkpointing to save memory
                scan_fn = jax.checkpoint(scan_step)
            else:
                scan_fn = scan_step
            
            _, trajectory = lax.scan(scan_fn, init_carry, scan_inputs)
            
            # åŒ…å«åˆå§‹çŠ¶æ€ / Include initial state
            full_trajectory = jnp.concatenate([
                jnp.expand_dims(x0, 0), trajectory
            ], axis=0)
            
            return full_trajectory
        
        return efficient_integrate
    
    else:
        # ç®€å•çš„å¾ªç¯ç‰ˆæœ¬ / Simple loop version
        def simple_integrate(params: NetworkParams, x0: SDEState,
                           time_grid: jnp.ndarray, key: jax.random.PRNGKey,
                           rngs: Dict[str, jax.random.PRNGKey]) -> jnp.ndarray:
            """Simple time integration / ç®€å•æ—¶é—´ç§¯åˆ†"""
            trajectory = [x0]
            x_curr = x0
            keys = jax.random.split(key, len(time_grid) - 1)
            
            for i in range(len(time_grid) - 1):
                t_curr = time_grid[i]
                dt = time_grid[i + 1] - t_curr
                
                drift = drift_fn(params, x_curr, t_curr, rngs, train=False)
                noise = jax.random.normal(keys[i], x_curr.shape)
                x_curr = x_curr + drift * dt + jnp.sqrt(dt) * noise
                trajectory.append(x_curr)
            
            return jnp.array(trajectory)
        
        return simple_integrate


# ============================================================================
# Training Utilities / è®­ç»ƒå·¥å…·
# ============================================================================

def create_training_state(
    network: nn.Module,
    config: TrainingConfig,
    key: jax.random.PRNGKey,
    input_shape: Tuple[int, ...],
    time_shape: Tuple[int, ...] = ()
) -> NetworkTrainingState:
    """
    Create training state for the network
    ä¸ºç½‘ç»œåˆ›å»ºè®­ç»ƒçŠ¶æ€
    
    Args:
        network: Neural network / ç¥ç»ç½‘ç»œ
        config: Training configuration / è®­ç»ƒé…ç½®
        key: Random key / éšæœºå¯†é’¥
        input_shape: Input shape / è¾“å…¥å½¢çŠ¶
        time_shape: Time shape / æ—¶é—´å½¢çŠ¶
        
    Returns:
        training_state: Network training state / ç½‘ç»œè®­ç»ƒçŠ¶æ€
    """
    # åˆå§‹åŒ–å‚æ•° / Initialize parameters
    params_key, dropout_key, key = jax.random.split(key, 3)
    dummy_x = jax.random.normal(params_key, input_shape)
    dummy_t = jnp.array(0.0) if not time_shape else jnp.zeros(time_shape)
    
    rngs = {'params': params_key, 'dropout': dropout_key}
    params = network.init(rngs, dummy_x, dummy_t, train=True)['params']
    
    # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆæ·»åŠ å­¦ä¹ ç‡é¢„çƒ­é˜²æ¢¯åº¦éœ‡è¡ï¼‰ / Create optimizer (with LR warmup to prevent gradient oscillation)
    warmup_schedule = optax.linear_schedule(
        init_value=0.0,
        end_value=config.learning_rate,
        transition_steps=config.warmup_steps
    )
    
    if config.decay_schedule == "cosine":
        decay_schedule = optax.cosine_decay_schedule(
            init_value=config.learning_rate,
            decay_steps=config.num_epochs * 1000,  # å‡è®¾æ¯è½®1000æ­¥
            alpha=0.1
        )
    else:
        decay_schedule = optax.exponential_decay(
            init_value=config.learning_rate,
            transition_steps=1000,
            decay_rate=0.9
        )
    
    # ç»„åˆé¢„çƒ­å’Œè¡°å‡è°ƒåº¦ / Combine warmup and decay schedules
    schedule = optax.join_schedules(
        schedules=[warmup_schedule, decay_schedule],
        boundaries=[config.warmup_steps]
    )
    
    optimizer = optax.chain(
        optax.clip_by_global_norm(config.gradient_clip_norm),
        optax.adamw(learning_rate=schedule)
    )
    
    if config.use_mixed_precision:
        # æ·»åŠ æ··åˆç²¾åº¦æ”¯æŒ / Add mixed precision support
        optimizer = optax.apply_if_finite(optimizer, max_consecutive_errors=5)
    
    opt_state = optimizer.init(params)
    
    return NetworkTrainingState(
        params=params,
        optimizer_state=opt_state,
        optimizer=optimizer,  # å­˜å‚¨ä¼˜åŒ–å™¨å®ä¾‹ / Store optimizer instance
        step=0,
        best_loss=float('inf'),
        metrics={}
    )


if __name__ == "__main__":
    # æµ‹è¯•ç½‘ç»œå®ç° / Test network implementation
    print("ğŸ§ª æµ‹è¯•FÃ¶llmer Drift Networks / Testing FÃ¶llmer Drift Networks")
    
    # åˆ›å»ºæµ‹è¯•é…ç½® / Create test configuration
    config = NetworkConfig(
        hidden_dims=[256, 256, 256],
        n_layers=4,
        activation="silu",
        use_attention=True,
        time_encoding_dim=128
    )
    
    # åˆ›å»ºç½‘ç»œ / Create network
    state_dim = 2
    network = FÃ¶llmerDriftNet(config=config, state_dim=state_dim)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­ / Test forward pass
    key = jax.random.PRNGKey(42)
    params_key, dropout_key = jax.random.split(key)
    x = jnp.array([1.0, 2.0])
    t = jnp.array(0.5)
    
    params = network.init({'params': params_key, 'dropout': dropout_key}, x, t, train=True)
    drift = network.apply(params, x, t, train=False)
    
    print(f"âœ… å•æ ·æœ¬æµ‹è¯•: x={x}, t={t}, drift={drift}")
    
    # æµ‹è¯•æ‰¹é‡å¤„ç† / Test batch processing
    batch_size = 64
    x_batch = jax.random.normal(key, (batch_size, state_dim))
    t_batch = jax.random.uniform(key, (batch_size,), minval=0.0, maxval=1.0)
    
    drift_batch = network.apply(params, x_batch, t_batch, train=False)
    
    print(f"âœ… æ‰¹é‡æµ‹è¯•: batch_size={batch_size}, output_shape={drift_batch.shape}")
    
    # æµ‹è¯•å¤šå°ºåº¦ç½‘ç»œ / Test multi-scale network
    multiscale_network = MultiScaleFÃ¶llmerDrift(config=config, state_dim=state_dim)
    ms_params_key, ms_dropout_key = jax.random.split(key)
    ms_params = multiscale_network.init({'params': ms_params_key, 'dropout': ms_dropout_key}, x, t, train=True)
    ms_drift = multiscale_network.apply(ms_params, x, t, train=False)
    
    print(f"âœ… å¤šå°ºåº¦æµ‹è¯•: drift={ms_drift}")
    
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼/ All tests passed!")
