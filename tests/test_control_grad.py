"""
Comprehensive test suite for Neural Control Variational Solver
ç¥ç»æ§åˆ¶å˜åˆ†æ±‚è§£å™¨çš„å…¨é¢æµ‹è¯•å¥—ä»¶

Tests cover:
- Mathematical correctness / æ•°å­¦æ­£ç¡®æ€§
- Performance and parallelization / æ€§èƒ½å’Œå¹¶è¡ŒåŒ–
- Training stability / è®­ç»ƒç¨³å®šæ€§
- Numerical stability / æ•°å€¼ç¨³å®šæ€§
- Integration with existing components / ä¸ç°æœ‰ç»„ä»¶çš„é›†æˆ
"""

import pytest
import jax
import jax.numpy as jnp
import jax.random as random
from jax import grad, value_and_grad
from functools import partial
import numpy as np
from typing import Dict, List, Tuple

# Import the modules to test / å¯¼å…¥è¦æµ‹è¯•çš„æ¨¡å—
from src.mmsbvi.algorithms.control_grad import (
    VariationalObjective, PathSampler, DensityEstimator, PrimalControlGradFlowSolver
)
from src.mmsbvi.core.types import (
    ControlGradConfig, ControlGradState, NetworkConfig, 
    SDEState, BatchStates, PathSamples
)
from src.mmsbvi.nets.flax_drift import FÃ¶llmerDriftNet


class TestControlGradConfig:
    """Test ControlGradConfig data structure / æµ‹è¯•ControlGradConfigæ•°æ®ç»“æ„"""
    
    def test_config_creation(self):
        """Test basic config creation / æµ‹è¯•åŸºæœ¬é…ç½®åˆ›å»º"""
        config = ControlGradConfig()
        assert config.state_dim == 2
        assert config.time_horizon == 1.0
        assert config.batch_size == 1024
        assert config.initial_params == {"mean": 0.0, "std": 1.0}
        assert config.target_params == {"mean": 0.0, "std": 1.0}
    
    def test_config_customization(self):
        """Test config customization / æµ‹è¯•é…ç½®å®šåˆ¶"""
        custom_config = ControlGradConfig(
            state_dim=3,
            batch_size=512,
            num_epochs=1000,
            initial_params={"mean": 1.0, "std": 2.0}
        )
        assert custom_config.state_dim == 3
        assert custom_config.batch_size == 512
        assert custom_config.num_epochs == 1000
        assert custom_config.initial_params["mean"] == 1.0
        assert custom_config.initial_params["std"] == 2.0


class TestVariationalObjective:
    """Test VariationalObjective component / æµ‹è¯•VariationalObjectiveç»„ä»¶"""
    
    @pytest.fixture
    def config(self):
        """Create test configuration / åˆ›å»ºæµ‹è¯•é…ç½®"""
        return ControlGradConfig(
            state_dim=2,
            batch_size=32,
            num_time_steps=10,
            time_horizon=1.0
        )
    
    @pytest.fixture
    def objective(self, config):
        """Create VariationalObjective instance / åˆ›å»ºVariationalObjectiveå®ä¾‹"""
        return VariationalObjective(config)
    
    @pytest.fixture
    def mock_network_apply(self):
        """Create mock network apply function / åˆ›å»ºæ¨¡æ‹Ÿç½‘ç»œåº”ç”¨å‡½æ•°"""
        def mock_apply(params, x, t, train):
            # Simple linear control: u(x,t) = -x / ç®€å•çº¿æ€§æ§åˆ¶
            return -x
        return mock_apply
    
    def test_integration_weights(self, objective):
        """Test trapezoidal integration weights / æµ‹è¯•æ¢¯å½¢ç§¯åˆ†æƒé‡"""
        weights = objective.integration_weights
        expected_length = objective.config.num_time_steps + 1
        assert len(weights) == expected_length
        
        # Check trapezoidal rule: first and last weights should be 0.5*dt
        # æ£€æŸ¥æ¢¯å½¢è§„åˆ™ï¼šç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæƒé‡åº”è¯¥æ˜¯0.5*dt
        dt = objective.dt
        assert jnp.allclose(weights[0], 0.5 * dt)
        assert jnp.allclose(weights[-1], 0.5 * dt)
        assert jnp.allclose(weights[1:-1], dt)
    
    def test_control_cost_computation(self, objective, mock_network_apply):
        """Test control cost computation / æµ‹è¯•æ§åˆ¶ä»£ä»·è®¡ç®—"""
        key = random.PRNGKey(42)
        batch_size = 4
        num_steps = objective.config.num_time_steps
        state_dim = objective.config.state_dim
        
        # Create sample paths / åˆ›å»ºæ ·æœ¬è·¯å¾„
        paths = random.normal(key, (batch_size, num_steps + 1, state_dim))
        times = jnp.linspace(0.0, 1.0, num_steps + 1)
        
        # Mock parameters / æ¨¡æ‹Ÿå‚æ•°
        params = {}
        
        cost = objective.compute_control_cost(
            paths, times, mock_network_apply, params, key
        )
        
        # Cost should be positive / ä»£ä»·åº”è¯¥ä¸ºæ­£
        assert cost >= 0.0
        assert jnp.isfinite(cost)
    
    def test_boundary_penalty_computation(self, objective):
        """Test boundary penalty computation / æµ‹è¯•è¾¹ç•Œæƒ©ç½šè®¡ç®—"""
        key = random.PRNGKey(42)
        batch_size = 4
        num_steps = objective.config.num_time_steps
        state_dim = objective.config.state_dim
        
        # Create sample paths / åˆ›å»ºæ ·æœ¬è·¯å¾„
        paths = random.normal(key, (batch_size, num_steps + 1, state_dim))
        
        # Create mock density functions / åˆ›å»ºæ¨¡æ‹Ÿå¯†åº¦å‡½æ•°
        def mock_initial_density(x):
            return jnp.sum(-0.5 * x**2, axis=-1)  # Gaussian log-density / é«˜æ–¯å¯¹æ•°å¯†åº¦
        
        def mock_target_density(x):
            return jnp.sum(-0.5 * x**2, axis=-1)  # Gaussian log-density / é«˜æ–¯å¯¹æ•°å¯†åº¦
        
        # Create mock density estimator for the updated function signature
        # ä¸ºæ›´æ–°çš„å‡½æ•°ç­¾ååˆ›å»ºæ¨¡æ‹Ÿå¯†åº¦ä¼°è®¡å™¨
        from src.mmsbvi.algorithms.control_grad import DensityEstimator
        config = ControlGradConfig(state_dim=state_dim)
        mock_density_estimator = DensityEstimator(config)
        
        penalty = objective.compute_boundary_penalty(
            paths, mock_initial_density, mock_target_density, mock_density_estimator
        )
        
        assert jnp.isfinite(penalty)
    
    def test_gradient_computation(self, objective, mock_network_apply):
        """Test gradient computation through objective / æµ‹è¯•é€šè¿‡ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦è®¡ç®—"""
        key = random.PRNGKey(42)
        batch_size = 4
        num_steps = objective.config.num_time_steps
        state_dim = objective.config.state_dim
        
        # Create sample paths / åˆ›å»ºæ ·æœ¬è·¯å¾„
        paths = random.normal(key, (batch_size, num_steps + 1, state_dim))
        times = jnp.linspace(0.0, 1.0, num_steps + 1)
        
        # Parametric network apply function / å‚æ•°åŒ–ç½‘ç»œåº”ç”¨å‡½æ•°
        def param_network_apply(params, x, t, train):
            # å¤„ç†å‚æ•°æ ¼å¼ï¼šå¦‚æœæ˜¯åµŒå¥—å­—å…¸æ ¼å¼ï¼Œæå–params / Handle parameter format
            if isinstance(params, dict) and "params" in params:
                actual_params = params["params"]
            else:
                actual_params = params
            return actual_params["weight"] * x
        
        # Mock density functions / æ¨¡æ‹Ÿå¯†åº¦å‡½æ•°
        def mock_density(x):
            return jnp.sum(-0.5 * x**2, axis=-1)
        
        # Test parameters / æµ‹è¯•å‚æ•°
        test_params = {"weight": jnp.array(1.0)}
        
        def loss_fn(params):
            cost = objective.compute_control_cost(
                paths, times, param_network_apply, params, key
            )
            # Create mock density estimator for updated function signature
            from src.mmsbvi.algorithms.control_grad import DensityEstimator
            config = ControlGradConfig(state_dim=objective.config.state_dim)
            mock_density_estimator = DensityEstimator(config)
            penalty = objective.compute_boundary_penalty(
                paths, mock_density, mock_density, mock_density_estimator
            )
            return cost + penalty
        
        # Compute gradients / è®¡ç®—æ¢¯åº¦
        loss_val, grads = value_and_grad(loss_fn)(test_params)
        
        assert jnp.isfinite(loss_val)
        assert "weight" in grads
        assert jnp.isfinite(grads["weight"])


class TestPathSampler:
    """Test PathSampler component / æµ‹è¯•PathSamplerç»„ä»¶"""
    
    @pytest.fixture
    def config(self):
        """Create test configuration / åˆ›å»ºæµ‹è¯•é…ç½®"""
        return ControlGradConfig(
            state_dim=2,
            batch_size=8,
            num_time_steps=20,
            time_horizon=1.0,
            diffusion_coeff=0.1
        )
    
    @pytest.fixture
    def path_sampler(self, config):
        """Create PathSampler instance / åˆ›å»ºPathSamplerå®ä¾‹"""
        return PathSampler(config)
    
    @pytest.fixture
    def mock_network_apply(self):
        """Create mock network apply function / åˆ›å»ºæ¨¡æ‹Ÿç½‘ç»œåº”ç”¨å‡½æ•°"""
        def mock_apply(params, x, t, train):
            return -0.5 * x  # Simple stabilizing control / ç®€å•ç¨³å®šæ§åˆ¶
        return mock_apply
    
    def test_initial_state_sampling_gaussian(self, path_sampler):
        """Test Gaussian initial state sampling / æµ‹è¯•é«˜æ–¯åˆå§‹çŠ¶æ€é‡‡æ ·"""
        key = random.PRNGKey(42)
        batch_size = 16
        
        states = path_sampler.sample_initial_states(
            batch_size, key, "gaussian", {"mean": 1.0, "std": 2.0}
        )
        
        assert states.shape == (batch_size, path_sampler.config.state_dim)
        
        # Check approximate mean and std / æ£€æŸ¥è¿‘ä¼¼å‡å€¼å’Œæ ‡å‡†å·®
        sample_mean = jnp.mean(states)
        sample_std = jnp.std(states)
        
        # Allow some tolerance for finite sampling / å…è®¸æœ‰é™é‡‡æ ·çš„ä¸€äº›å®¹å·®
        assert jnp.abs(sample_mean - 1.0) < 0.5
        assert jnp.abs(sample_std - 2.0) < 0.5
    
    def test_initial_state_sampling_uniform(self, path_sampler):
        """Test uniform initial state sampling / æµ‹è¯•å‡åŒ€åˆå§‹çŠ¶æ€é‡‡æ ·"""
        key = random.PRNGKey(42)
        batch_size = 16
        
        states = path_sampler.sample_initial_states(
            batch_size, key, "uniform", {"low": -2.0, "high": 2.0}
        )
        
        assert states.shape == (batch_size, path_sampler.config.state_dim)
        assert jnp.all(states >= -2.0)
        assert jnp.all(states <= 2.0)
    
    def test_controlled_path_sampling(self, path_sampler, mock_network_apply):
        """Test controlled path sampling / æµ‹è¯•æ§åˆ¶è·¯å¾„é‡‡æ ·"""
        key = random.PRNGKey(42)
        batch_size = 4
        state_dim = path_sampler.config.state_dim
        
        # Initial states / åˆå§‹çŠ¶æ€
        initial_states = random.normal(key, (batch_size, state_dim))
        
        # Sample paths / é‡‡æ ·è·¯å¾„
        paths = path_sampler.sample_controlled_paths(
            initial_states, key, mock_network_apply, {}
        )
        
        expected_shape = (batch_size, path_sampler.config.num_time_steps + 1, state_dim)
        assert paths.shape == expected_shape
        
        # Check initial conditions preserved / æ£€æŸ¥åˆå§‹æ¡ä»¶ä¿æŒ
        assert jnp.allclose(paths[:, 0, :], initial_states, atol=1e-6)
        
        # Check no NaN or Inf / æ£€æŸ¥æ²¡æœ‰NaNæˆ–Inf
        assert jnp.all(jnp.isfinite(paths))
    
    def test_path_sampling_determinism(self, path_sampler, mock_network_apply):
        """Test deterministic behavior with same random key / æµ‹è¯•ç›¸åŒéšæœºå¯†é’¥çš„ç¡®å®šæ€§è¡Œä¸º"""
        key = random.PRNGKey(123)
        batch_size = 4
        state_dim = path_sampler.config.state_dim
        
        initial_states = random.normal(key, (batch_size, state_dim))
        
        # Sample twice with same key / ä½¿ç”¨ç›¸åŒå¯†é’¥é‡‡æ ·ä¸¤æ¬¡
        paths1 = path_sampler.sample_controlled_paths(
            initial_states, key, mock_network_apply, {}
        )
        paths2 = path_sampler.sample_controlled_paths(
            initial_states, key, mock_network_apply, {}
        )
        
        # Should be identical / åº”è¯¥ç›¸åŒ
        assert jnp.allclose(paths1, paths2, atol=1e-10)


class TestDensityEstimator:
    """Test DensityEstimator component / æµ‹è¯•DensityEstimatorç»„ä»¶"""
    
    @pytest.fixture
    def config(self):
        """Create test configuration / åˆ›å»ºæµ‹è¯•é…ç½®"""
        return ControlGradConfig(state_dim=2)
    
    @pytest.fixture
    def estimator(self, config):
        """Create DensityEstimator instance / åˆ›å»ºDensityEstimatorå®ä¾‹"""
        return DensityEstimator(config)
    
    def test_gaussian_density_creation(self, estimator):
        """Test Gaussian density function creation / æµ‹è¯•é«˜æ–¯å¯†åº¦å‡½æ•°åˆ›å»º"""
        mean = 1.0
        std = 2.0
        
        density_fn = estimator.create_gaussian_density_fn(mean, std)
        
        # Test evaluation / æµ‹è¯•è¯„ä¼°
        test_points = jnp.array([[1.0, 1.0], [0.0, 0.0]])
        log_densities = jax.vmap(density_fn)(test_points)
        
        assert log_densities.shape == (2,)
        assert jnp.all(jnp.isfinite(log_densities))
        
        # Point at mean should have higher density than point far away
        # å‡å€¼å¤„çš„ç‚¹åº”è¯¥æ¯”è¿œå¤„çš„ç‚¹å…·æœ‰æ›´é«˜çš„å¯†åº¦
        assert log_densities[0] > log_densities[1]
    
    def test_kde_density_creation(self, estimator):
        """Test KDE density function creation / æµ‹è¯•KDEå¯†åº¦å‡½æ•°åˆ›å»º"""
        key = random.PRNGKey(42)
        n_samples = 100
        state_dim = estimator.config.state_dim
        
        # Generate sample data / ç”Ÿæˆæ ·æœ¬æ•°æ®
        samples = random.normal(key, (n_samples, state_dim))
        
        density_fn = estimator.create_kde_density_fn(samples, "scott")
        
        # Test evaluation / æµ‹è¯•è¯„ä¼°
        test_points = jnp.array([[0.0, 0.0], [5.0, 5.0]])
        log_densities = jax.vmap(density_fn)(test_points)
        
        assert log_densities.shape == (2,)
        assert jnp.all(jnp.isfinite(log_densities))
        
        # Point near samples should have higher density
        # é è¿‘æ ·æœ¬çš„ç‚¹åº”è¯¥æœ‰æ›´é«˜çš„å¯†åº¦
        assert log_densities[0] > log_densities[1]


class TestPrimalControlGradFlowSolver:
    """Test main solver / æµ‹è¯•ä¸»æ±‚è§£å™¨"""
    
    @pytest.fixture
    def config(self):
        """Create test configuration / åˆ›å»ºæµ‹è¯•é…ç½®"""
        return ControlGradConfig(
            state_dim=2,
            batch_size=16,
            num_epochs=5,  # Small number for testing / æµ‹è¯•ç”¨å°æ•°é‡
            num_time_steps=10,
            learning_rate=1e-3
        )
    
    @pytest.fixture
    def network_config(self):
        """Create network configuration / åˆ›å»ºç½‘ç»œé…ç½®"""
        return NetworkConfig(
            hidden_dims=[32, 32],  # Small network for testing / æµ‹è¯•ç”¨å°ç½‘ç»œ
            n_layers=2,
            activation="silu",
            use_attention=False,
            dropout_rate=0.0,
            time_encoding_dim=16
        )
    
    @pytest.fixture
    def solver(self, config, network_config):
        """Create solver instance / åˆ›å»ºæ±‚è§£å™¨å®ä¾‹"""
        return PrimalControlGradFlowSolver(config, network_config)
    
    def test_solver_initialization(self, solver):
        """Test solver initialization / æµ‹è¯•æ±‚è§£å™¨åˆå§‹åŒ–"""
        assert solver.config is not None
        assert solver.network_config is not None
        assert solver.objective is not None
        assert solver.path_sampler is not None
        assert solver.density_estimator is not None
    
    def test_network_initialization(self, solver):
        """Test network initialization / æµ‹è¯•ç½‘ç»œåˆå§‹åŒ–"""
        key = random.PRNGKey(42)
        
        training_state = solver.initialize_network(key)
        
        assert training_state is not None
        assert training_state.params is not None
        assert training_state.step == 0
        assert solver.network is not None
    
    def test_single_training_step(self, solver):
        """Test single training step / æµ‹è¯•å•ä¸ªè®­ç»ƒæ­¥éª¤"""
        key = random.PRNGKey(42)
        
        # Initialize network / åˆå§‹åŒ–ç½‘ç»œ
        training_state = solver.initialize_network(key)
        
        # Create solver state with JAX arrays for history (matching updated structure)
        # ä½¿ç”¨JAXæ•°ç»„åˆ›å»ºæ±‚è§£å™¨çŠ¶æ€ç”¨äºå†å²è®°å½•ï¼ˆåŒ¹é…æ›´æ–°çš„ç»“æ„ï¼‰
        max_epochs = solver.config.num_epochs
        state = ControlGradState(
            training_state=training_state,
            config=solver.config,
            step=0,
            epoch=0,
            best_loss=float('inf'),
            loss_history=jnp.full(max_epochs, jnp.nan),
            gradient_norm_history=jnp.full(max_epochs, jnp.nan),
            time_per_epoch=jnp.full(max_epochs, jnp.nan),
            control_cost_history=jnp.full(max_epochs, jnp.nan),
            boundary_penalty_history=jnp.full(max_epochs, jnp.nan),
            history_index=0
        )
        
        # Sample initial states / é‡‡æ ·åˆå§‹çŠ¶æ€
        batch_key, train_key = random.split(key)
        batch_initial_states = solver.path_sampler.sample_initial_states(
            solver.config.batch_size,
            batch_key,
            solver.config.initial_distribution,
            solver.config.initial_params
        )
        
        # Perform training step / æ‰§è¡Œè®­ç»ƒæ­¥éª¤
        new_state, metrics = solver.train_step(state, batch_initial_states, train_key)
        
        # Check results with JAX array history structure
        # æ£€æŸ¥JAXæ•°ç»„å†å²ç»“æ„çš„ç»“æœ
        assert new_state.step == 1
        assert new_state.best_loss <= state.best_loss
        assert new_state.history_index == 1  # One entry recorded
        assert jnp.isfinite(new_state.loss_history[0])  # First entry should be finite
        assert jnp.isnan(new_state.loss_history[1])  # Subsequent entries should be NaN
        assert jnp.isfinite(new_state.gradient_norm_history[0])
        assert jnp.isnan(new_state.gradient_norm_history[1])
        
        # Check metrics / æ£€æŸ¥æŒ‡æ ‡
        assert "total_loss" in metrics
        assert "control_cost" in metrics
        assert "boundary_penalty" in metrics
        assert "gradient_norm" in metrics
        
        assert jnp.isfinite(metrics["total_loss"])
        assert jnp.isfinite(metrics["control_cost"])
        assert jnp.isfinite(metrics["gradient_norm"])
        assert metrics["control_cost"] >= 0.0
    
    def test_short_training_loop(self, solver):
        """Test short training loop / æµ‹è¯•çŸ­è®­ç»ƒå¾ªç¯"""
        key = random.PRNGKey(42)
        
        # Run very short training / è¿è¡Œå¾ˆçŸ­çš„è®­ç»ƒ
        final_state = solver.train(key)
        
        # Check final state / æ£€æŸ¥æœ€ç»ˆçŠ¶æ€
        assert final_state.step > 0
        assert final_state.epoch == solver.config.num_epochs - 1
        assert len(final_state.loss_history) == solver.config.num_epochs
        assert len(final_state.gradient_norm_history) == solver.config.num_epochs
        assert final_state.best_loss < float('inf')


class TestNumericalStability:
    """Test numerical stability / æµ‹è¯•æ•°å€¼ç¨³å®šæ€§"""
    
    def test_extreme_values_handling(self):
        """Test handling of extreme values / æµ‹è¯•æå€¼å¤„ç†"""
        config = ControlGradConfig(state_dim=2, batch_size=4, num_time_steps=2)  # åŒ¹é…è·¯å¾„é•¿åº¦ / Match path length
        objective = VariationalObjective(config)
        
        # Create paths with extreme values / åˆ›å»ºå…·æœ‰æå€¼çš„è·¯å¾„
        extreme_paths = jnp.array([
            [[1e6, 1e6], [1e6, 1e6], [1e6, 1e6]],  # Very large values / å¾ˆå¤§çš„å€¼
            [[-1e6, -1e6], [-1e6, -1e6], [-1e6, -1e6]],  # Very small values / å¾ˆå°çš„å€¼
            [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],  # Zero values / é›¶å€¼
            [[1e-10, 1e-10], [1e-10, 1e-10], [1e-10, 1e-10]]  # Tiny values / å¾®å°å€¼
        ])
        
        times = jnp.linspace(0.0, 1.0, 3)
        
        def stable_network_apply(params, x, t, train):
            return jnp.tanh(x)  # Bounded output / æœ‰ç•Œè¾“å‡º
        
        key = random.PRNGKey(42)
        cost = objective.compute_control_cost(
            extreme_paths, times, stable_network_apply, {}, key
        )
        
        # Should not be NaN or Inf / ä¸åº”è¯¥æ˜¯NaNæˆ–Inf
        assert jnp.isfinite(cost)
    
    def test_gradient_numerical_stability(self):
        """Test gradient numerical stability / æµ‹è¯•æ¢¯åº¦æ•°å€¼ç¨³å®šæ€§"""
        config = ControlGradConfig(state_dim=1, batch_size=2, num_time_steps=5)
        objective = VariationalObjective(config)
        
        def param_network_apply(params, x, t, train):
            # å¤„ç†å‚æ•°æ ¼å¼ï¼šå¦‚æœæ˜¯åµŒå¥—å­—å…¸æ ¼å¼ï¼Œæå–params / Handle parameter format
            if isinstance(params, dict) and "params" in params:
                actual_params = params["params"]
            else:
                actual_params = params
            return actual_params["scale"] * x
        
        def mock_density(x):
            return -0.5 * jnp.sum(x**2, axis=-1)
        
        key = random.PRNGKey(42)
        paths = random.normal(key, (2, 6, 1))
        times = jnp.linspace(0.0, 1.0, 6)
        
        def loss_fn(params):
            cost = objective.compute_control_cost(
                paths, times, param_network_apply, params, key
            )
            # Create mock density estimator for updated function signature
            from src.mmsbvi.algorithms.control_grad import DensityEstimator
            config = ControlGradConfig(state_dim=objective.config.state_dim)
            mock_density_estimator = DensityEstimator(config)
            penalty = objective.compute_boundary_penalty(
                paths, mock_density, mock_density, mock_density_estimator
            )
            return cost + penalty
        
        # Test different parameter scales / æµ‹è¯•ä¸åŒçš„å‚æ•°å°ºåº¦
        for scale in [1e-6, 1e-3, 1.0, 1e3, 1e6]:
            test_params = {"scale": jnp.array(scale)}
            
            try:
                loss_val, grads = value_and_grad(loss_fn)(test_params)
                
                # Gradients should be finite / æ¢¯åº¦åº”è¯¥æ˜¯æœ‰é™çš„
                assert jnp.isfinite(loss_val)
                assert jnp.isfinite(grads["scale"])
                
            except Exception as e:
                pytest.fail(f"Gradient computation failed for scale {scale}: {e}")


class TestPerformanceAndParallelization:
    """Test performance and parallelization / æµ‹è¯•æ€§èƒ½å’Œå¹¶è¡ŒåŒ–"""
    
    def test_batch_processing_consistency(self):
        """Test batch processing gives consistent results / æµ‹è¯•æ‰¹å¤„ç†ç»™å‡ºä¸€è‡´çš„ç»“æœ"""
        config = ControlGradConfig(state_dim=2, num_time_steps=5)
        path_sampler = PathSampler(config)
        
        def mock_network_apply(params, x, t, train):
            return -0.1 * x
        
        key = random.PRNGKey(42)
        
        # Test single sample / æµ‹è¯•å•æ ·æœ¬
        single_initial = random.normal(key, (1, 2))
        single_path = path_sampler.sample_controlled_paths(
            single_initial, key, mock_network_apply, {}
        )
        
        # Test batch of same initial state / æµ‹è¯•ç›¸åŒåˆå§‹çŠ¶æ€çš„æ‰¹æ¬¡
        batch_initial = jnp.tile(single_initial, (4, 1))
        batch_paths = path_sampler.sample_controlled_paths(
            batch_initial, key, mock_network_apply, {}
        )
        
        # All paths in batch should be identical to single path
        # æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰è·¯å¾„éƒ½åº”è¯¥ä¸å•è·¯å¾„ç›¸åŒ
        for i in range(4):
            assert jnp.allclose(batch_paths[i:i+1], single_path, atol=1e-10)
    
    def test_memory_usage_batch_scaling(self):
        """Test memory doesn't explode with batch size / æµ‹è¯•å†…å­˜ä¸ä¼šéšæ‰¹æ¬¡å¤§å°çˆ†ç‚¸"""
        config = ControlGradConfig(state_dim=2, num_time_steps=10)
        objective = VariationalObjective(config)
        
        def simple_network_apply(params, x, t, train):
            return x * 0.1
        
        key = random.PRNGKey(42)
        times = jnp.linspace(0.0, 1.0, 11)
        
        # Test different batch sizes / æµ‹è¯•ä¸åŒçš„æ‰¹æ¬¡å¤§å°
        for batch_size in [1, 4, 16, 64]:
            paths = random.normal(key, (batch_size, 11, 2))
            
            cost = objective.compute_control_cost(
                paths, times, simple_network_apply, {}, key
            )
            
            # Should be able to handle all batch sizes / åº”è¯¥èƒ½å¤Ÿå¤„ç†æ‰€æœ‰æ‰¹æ¬¡å¤§å°
            assert jnp.isfinite(cost)
            assert cost >= 0.0


if __name__ == "__main__":
    # Run tests / è¿è¡Œæµ‹è¯•
    print("ğŸ§ª è¿è¡ŒNeural Control Variationalæµ‹è¯•å¥—ä»¶ / Running Neural Control Variational test suite")
    
    # Simple smoke test / ç®€å•å†’çƒŸæµ‹è¯•
    config = ControlGradConfig(
        state_dim=2,
        batch_size=8,
        num_epochs=3,
        num_time_steps=5
    )
    
    print("âœ… é…ç½®æµ‹è¯•é€šè¿‡ / Configuration test passed")
    
    # Test components / æµ‹è¯•ç»„ä»¶
    objective = VariationalObjective(config)
    path_sampler = PathSampler(config)
    estimator = DensityEstimator(config)
    
    print("âœ… ç»„ä»¶åˆ›å»ºæµ‹è¯•é€šè¿‡ / Component creation test passed")
    
    # Test basic functionality / æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    key = random.PRNGKey(42)
    initial_states = path_sampler.sample_initial_states(4, key)
    
    def mock_apply(params, x, t, train):
        return -0.5 * x
    
    paths = path_sampler.sample_controlled_paths(initial_states, key, mock_apply, {})
    
    print(f"âœ… è·¯å¾„é‡‡æ ·æµ‹è¯•é€šè¿‡ï¼Œå½¢çŠ¶: {paths.shape} / Path sampling test passed, shape: {paths.shape}")
    
    # Test objective computation / æµ‹è¯•ç›®æ ‡è®¡ç®—
    times = jnp.linspace(0.0, 1.0, config.num_time_steps + 1)
    cost = objective.compute_control_cost(paths, times, mock_apply, {}, key)
    
    print(f"âœ… æ§åˆ¶ä»£ä»·è®¡ç®—é€šè¿‡ï¼Œå€¼: {cost:.6f} / Control cost computation passed, value: {cost:.6f}")
    
    print("ğŸ‰ æ‰€æœ‰åŸºæœ¬æµ‹è¯•é€šè¿‡ï¼/ All basic tests passed!")